#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¼ì¼ ê³¼í•™ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ìµœì¢… ìˆ˜ì •íŒ)
- ZoneInfoë¥¼ í†µí•œ ì •í™•í•œ Palo Alto ì‹œê°„ ì ìš©
- AI ì‘ë‹µ(List/Dict) ìœ ì—°í•œ ì²˜ë¦¬ (ì—ëŸ¬ ë°©ì§€)
- API ë° ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ì— ëŒ€í•œ ê°•í•œ ë‚´ì„± ë° ë¡œê¹…
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import feedparser
import time
import os
from google import genai
from google.genai import types
import sys
import re
# íƒ€ì„ì¡´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Python 3.9+)
try:
    from zoneinfo import ZoneInfo
except ImportError:
    # êµ¬ë²„ì „ íŒŒì´ì¬ ëŒ€ë¹„ (GitHub ActionsëŠ” ë³´í†µ ìµœì‹ ì„)
    from datetime import timezone, timedelta
    class ZoneInfo:
        def __init__(self, key): pass
        def utcoffset(self, dt): return timedelta(hours=-8)
        def tzname(self, dt): return "PST"
        def dst(self, dt): return timedelta(0)

# ============================================================================
# ì„¤ì •
# ============================================================================

MAX_NEW_ARTICLES_PER_RUN = 10000
API_DELAY_SECONDS = 2 # API ì•ˆì •ì„±ì„ ìœ„í•´ 1ì´ˆ -> 2ì´ˆë¡œ ëŠ˜ë¦¼
# ì¬ì‹œë„ íšŸìˆ˜ ì„¤ì • (ì´ 3ë²ˆ ì‹œë„)
max_retries = 5

# íŒ”ë¡œì•Œí†  ì‹œê°„ëŒ€ (ì¸ë¨¸íƒ€ì„ ìë™ ì ìš©)
try:
    PALO_ALTO_TZ = ZoneInfo("America/Los_Angeles")
except:
    PALO_ALTO_TZ = timezone(timedelta(hours=-8)) # fallback

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'application/xml,application/rss+xml,text/xml;q=0.9,*/*;q=0.5'
}

# ì‹¤í–‰ ë¡œê·¸ ì „ì—­ ë³€ìˆ˜
execution_logs = []

def log(message, level="INFO"):
    """ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤."""
    now = datetime.now(PALO_ALTO_TZ)
    timestamp = now.strftime('%H:%M:%S')
    
    # ì½˜ì†” ì¶œë ¥ (GitHub Actions ë¡œê·¸ìš©)
    print(f"[{timestamp}] [{level}] {message}")
    
    # íŒŒì¼ ì €ì¥ìš© ë¡œê·¸ (ì›¹ í‘œì‹œìš©)
    execution_logs.append({
        "time": timestamp,
        "level": level,
        "message": message
    })

# ============================================================================
# AI ë²ˆì—­ ë° ìš”ì•½
# ============================================================================

def clean_json_text(text):
    """JSON ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ì œ"""
    text = re.sub(r'^```json\s*', '', text)
    text = re.sub(r'^```\s*', '', text)
    text = re.sub(r'\s*```$', '', text)
    return text.strip()

def get_gemini_summary(article_data, model_id):
    
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ì½˜í…ì¸ ë¥¼ ë²ˆì—­í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.
    ìœ íŠœë¸Œ ì˜ìƒì˜ ê²½ìš° URLì„ í†µí•´ ì§ì ‘ ì˜ìƒ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        article_data (dict): title_en, description_en, url, sourceë¥¼ í¬í•¨í•œ ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°
        
    Returns:
        tuple: (translated_title_kr, summary_kr)
    """
    
    title_en = article_data['title_en']
    description_en = article_data['description_en']
    url = article_data['url']
    source = article_data.get('source', '')
    
    api_key = os.environ.get('GEMINI_API_KEY')
    
    if not api_key:
        print("  [AI] âŒ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í‚¤ ì—†ìŒ. (ì›ë³¸: {description_en[:100]}...)"
    
    client = genai.Client(api_key=api_key)
    
    for attempt in range(max_retries):
        try:        
            # í˜„ì¬ ì‹œë„ ì¤‘ì¸ ëª¨ë¸ ì¶œë ¥
            print(f"  [AI] ğŸ¤– ëª¨ë¸ ì—°ê²° ì‹œë„: {model_id} ...")
            time.sleep(5)
            # ìœ íŠœë¸Œ ì˜ìƒ: URLì„ í†µí•´ ì§ì ‘ ì˜ìƒ ì½˜í…ì¸  ë¶„ì„
            if 'YouTube' in source:
                print(f"  [AI] ğŸ¥ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ ì¤‘: '{title_en[:40]}...'")
                
                prompt = f"""
                        ë‹¹ì‹ ì€ ì˜ìƒ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ ìœ íŠœë¸Œ ì˜ìƒì„ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.
                        ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
                        
                        [ì…ë ¥]
                        - title_en: "{title_en}"
                        
                        [JSON ì¶œë ¥ í˜•ì‹]
                        {{
                          "title_kr": "ì—¬ê¸°ì— ì œëª©ì˜ ì „ë¬¸ì ì¸ í•œêµ­ì–´ ë²ˆì—­ì„ ì‘ì„±í•©ë‹ˆë‹¤",
                          "summary_kr": "í•µì‹¬ ìš”ì ì„ ì¶”ì¶œí•˜ì—¬, ì˜ìƒ ì½˜í…ì¸ ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ìµœì†Œ 10ë¬¸ì¥ ë¶„ëŸ‰ì˜ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤"
                        }}
                        
                        [ê·œì¹™]
                        1. "title_kr": "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
                        2. "summary_kr": ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì²´ë¡œ ìƒì„¸í•œ ìµœì†Œ 10ë¬¸ì¥ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
                        3. ëŒ€í™”ì²´ê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ê¸€ì“°ê¸° ë¬¸ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                        """
    
                response = client.models.generate_content(
                    model = model_id,
                    contents=[
                        prompt,
                        types.Part.from_uri(
                            file_uri=url,
                            mime_type="video/youtube"
                        )
                    ],
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json"
                    )
                )
                
            # í…ìŠ¤íŠ¸ ê¸°ì‚¬: ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ë²ˆì—­ ë° ìš”ì•½
            else:
                print(f"  [AI] ğŸ“ ê¸°ì‚¬ ë²ˆì—­ ì¤‘: '{title_en[:40]}...'")
                
                prompt = f"""
                        ë‹¹ì‹ ì€ ê³¼í•™ì— ëŠ¥í†µí•œ ì „ë¬¸ ê¸°ì í˜¹ì€ ì»¤ë®¤ë‹ˆì¼€ì´í„°ì…ë‹ˆë‹¤.
                        ì•„ë˜ì˜ ì˜ì–´ ê¸°ì‚¬ ì œëª©ê³¼ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ, í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
                        ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
                         
                        [ì…ë ¥]
                        - title_en: "{title_en}"
                        - description_en: "{description_en}"
                        
                        [JSON ì¶œë ¥ í˜•ì‹]
                        {{
                          "title_kr": "ì—¬ê¸°ì— í•œêµ­ì–´ ë²ˆì—­ ì œëª©ì„ ì‘ì„±",
                          "summary_kr": "ì—¬ê¸°ì— ìµœì†Œ 5-6 ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ìƒì„¸í•œ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±"
                        }}
                        
                        [ê·œì¹™]
                        1. "title_kr" í‚¤ì—ëŠ” "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
                        2. "summary_kr" í‚¤ì—ëŠ” "description_en"ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
                        3. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì²´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
                        """
                
                response = client.models.generate_content(
                    model = model_id,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json"
                    )
                )
    
    
            # [ìˆ˜ì •ëœ ë¶€ë¶„] í…ìŠ¤íŠ¸ ì •ì œ (ë§ˆí¬ë‹¤ìš´ ì œê±°)
            text = response.text
            if text.startswith("```"):
                text = re.sub(r"^```json\s*", "", text) # ì‹œì‘ ë¶€ë¶„ ```json ì œê±°
                text = re.sub(r"^```\s*", "", text)     # ì‹œì‘ ë¶€ë¶„ ``` ì œê±°
                text = re.sub(r"\s*```$", "", text)     # ë ë¶€ë¶„ ``` ì œê±°
            
            text = text.strip() # ì•ë’¤ ê³µë°± ì œê±°
    
            # JSON íŒŒì‹±
            data = json.loads(text, strict=False)
            
            title_kr = data.get('title_kr', title_en)
            summary_kr = data.get('summary_kr', "ìš”ì•½ ë‚´ìš© ì—†ìŒ")
    
            log(f"  [AI] âœ… ì™„ë£Œ: {title_kr[:20]}...")
            return title_kr, summary_kr
    
        # ğŸš¨ ì—ëŸ¬ ì²˜ë¦¬ í•¸ë“¤ëŸ¬ (ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ê¸¸ì§€ ê²°ì •)
        except Exception as e:
            error_msg = str(e)
            
            # 1. í• ë‹¹ëŸ‰ ì´ˆê³¼ (429) ë˜ëŠ” ëª¨ë¸ ì—†ìŒ (404) -> ë‹¤ìŒ ëª¨ë¸ë¡œ Pass
            if "429" in error_msg or "404" in error_msg or "ResourceExhausted" in error_msg or "Not Found" in error_msg:
                print(f"  [AI] âš ï¸ {model_id} ì‚¬ìš© ë¶ˆê°€ (í•œë„ì´ˆê³¼/ë¯¸ì§€ì›). (ì‹œë„ {attempt+1}): {e}")
                return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] {str(e)}"
        
        except json.JSONDecodeError as e:
            print(f"  [AI] âŒ JSON íŒŒì‹± ì—ëŸ¬: {e}")
            print(f"  [ë””ë²„ê·¸] ë¬¸ì œì˜ í…ìŠ¤íŠ¸: {response.text[:100]}...") # ë””ë²„ê¹…ìš© ì¶œë ¥
            return title_en, "[ìš”ì•½ ì‹¤íŒ¨] AI ì‘ë‹µ ì˜¤ë¥˜ (JSON íŒŒì‹± ì‹¤íŒ¨)"
        
        except Exception as e:
            # [ìˆ˜ì •] ëŒ€ê¸° ì‹œê°„ ì ì§„ì  ì¦ê°€ (2ì´ˆ -> 4ì´ˆ -> 8ì´ˆ -> 16ì´ˆ...)
            wait_time = 2 * (2 ** attempt) 
            print(f"  [AI] âš ï¸ ì—ëŸ¬ ë°œìƒ (ì‹œë„ {attempt+1}): {e}")
            
            if attempt < max_retries - 1:
                print(f"  â³ {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(wait_time)
                continue
            else:
                print(f"  [AI] âŒ ìµœì¢… ì‹¤íŒ¨: {title_en[:20]}...")
                return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] {str(e)}"
                
# ============================================================================
# ìŠ¤í¬ë˜í¼
# ============================================================================

def scrape_feed(feed_url, source_name, category_name):
    articles = []
    log(f"í¬ë¡¤ë§ ì‹œì‘: {source_name}", "INFO")

    try:
        response = requests.get(feed_url, headers=HEADERS, timeout=10)
        feed = feedparser.parse(response.content)
        
        # [í•µì‹¬] ì´ ì‹œê°„ì´ ê³§ ê¸°ì‚¬ì˜ ë‚ ì§œê°€ ë©ë‹ˆë‹¤.
        palo_alto_now = datetime.now(PALO_ALTO_TZ)
        date_str = palo_alto_now.strftime('%Y-%m-%d')

        for entry in feed.entries:
            link = entry.get('link')
            title = entry.get('title')
            
            if not link or not title:
                continue

            # [ì‚­ì œë¨] ê¸°ì¡´ì˜ published_parsed ë¡œì§ ì œê±°
            # ë¬´ì¡°ê±´ ìœ„ì—ì„œ ì •ì˜í•œ date_str(ì˜¤ëŠ˜)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            image_url = None
            if entry.get('media_thumbnail'):
                image_url = entry.media_thumbnail[0]['url']
            elif entry.get('links'):
                for lk in entry.links:
                    if lk.get('type', '').startswith('image/'):
                        image_url = lk.get('href')
                        break
            
            # ë‚´ìš© ì¶”ì¶œ
            desc = entry.get('summary', '')
            clean_desc = BeautifulSoup(desc, 'html.parser').get_text(strip=True)

            articles.append({
                'title_en': title,
                'description_en': clean_desc,
                'url': link,
                'source': source_name,
                'category': category_name,
                'date': date_str, # ë¬´ì¡°ê±´ ì˜¤ëŠ˜ ë‚ ì§œ
                'image_url': image_url
            })
            
        log(f"[{source_name}] ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨", "INFO")

    except requests.exceptions.Timeout:
        log(f"{source_name}: ì—°ê²° ì‹œê°„ ì´ˆê³¼ (Timeout)", "ERROR")
    except Exception as e:
        log(f"{source_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")

    return articles
# ============================================================================
# ìœ íŠœë¸Œ ì±„ë„ ìŠ¤í¬ë˜í¼
# ============================================================================

def scrape_youtube_videos(channel_id, source_name, category_name):
    articles = []
    log(f"ğŸ” [{source_name}] ìœ íŠœë¸Œ í¬ë¡¤ë§ ì¤‘... (ì±„ë„: {channel_id})")
    feed_url = f'https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}'

    try:
        response = requests.get(feed_url, headers=HEADERS, timeout=20)
        response.raise_for_status()
        feed = feedparser.parse(response.content)

        # [í•µì‹¬] ìœ íŠœë¸Œë„ ë¬´ì¡°ê±´ ì˜¤ëŠ˜(Palo Alto) ë‚ ì§œë¡œ ê³ ì •
        palo_alto_now = datetime.now(PALO_ALTO_TZ)
        date_str = palo_alto_now.strftime('%Y-%m-%d')

        print(f"  [i] {len(feed.entries)}ê°œì˜ ìµœì‹  ì˜ìƒ ë°œê²¬")

        for entry in feed.entries:
            try:
                if not entry.get('title') or not entry.get('link'):
                    continue

                title_en = entry.title
                link = entry.link
                video_id = link.split('v=')[-1]

                # [ì‚­ì œë¨] ê¸°ì¡´ì˜ published_parsed ë¡œì§ ì œê±°
                
                # ê³ í™”ì§ˆ ì¸ë„¤ì¼
                image_url = None
                if entry.get('media_thumbnail') and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0]['url'].replace('default.jpg', 'hqdefault.jpg')

                description_en = entry.get('media_description', entry.get('summary', title_en))
                description_text = BeautifulSoup(description_en, 'html.parser').get_text(strip=True)
                
                log(f"    [i] ì˜ìƒ {video_id} ë¡œë“œë¨.")

                articles.append({
                    'title_en': title_en,
                    'description_en': description_text,
                    'url': link,
                    'source': source_name,
                    'category': category_name,
                    'date': date_str, # ë¬´ì¡°ê±´ ì˜¤ëŠ˜ ë‚ ì§œ
                    'image_url': image_url
                })

            except Exception as item_err:
                log(f"  âœ— ì˜ìƒ íŒŒì‹± ì‹¤íŒ¨: {item_err}")

    except Exception as e:
        log(f"âŒ [{source_name}] ì˜¤ë¥˜: {e}")

    return articles

# %%
# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    start_time = datetime.now(PALO_ALTO_TZ)
    log(f"=== ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ (ë‚ ì§œ: {start_time.strftime('%Y-%m-%d')}) ===", "INFO")

    # 1. ë°ì´í„° ë¡œë“œ
    seen_urls = set()
    old_articles = []
    failed_queue = []

    try:
        with open('articles.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # [í•µì‹¬] ë‚ ì§œ ì œí•œ ì—†ì´ ëª¨ë“  ê³¼ê±° ê¸°ë¡ì„ ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ìš©)
            for art in data.get('articles', []):
                # URLì´ ìˆëŠ” ìœ íš¨í•œ ê¸°ì‚¬ë§Œ ë¡œë“œ
                if art.get('url'):
                    old_articles.append(art)
                    seen_urls.add(art['url'])
            
            # ì´ì „ ì‹¤íŒ¨ ëª©ë¡ ë¡œë“œ
            failed_queue = data.get('failed_queue', [])
            
    except FileNotFoundError:
        log("ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ì—†ìŒ. ìƒˆë¡œ ì‹œì‘.", "WARNING")
    except json.JSONDecodeError:
        log("JSON íŒŒì¼ ê¹¨ì§. ë°±ì—… í›„ ìƒˆë¡œ ì‹œì‘.", "ERROR")
    except Exception as e:
        log(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}", "ERROR")

    # 2. ìˆ˜ì§‘ ì†ŒìŠ¤ ì •ì˜
    sources = [
        # General
        ('https://www.technologyreview.com/feed/', 'MIT Tech Rev', 'News'),
        ('https://www.nature.com/nature/rss/articles?type=news', 'Nature', 'News'),
        ('https://www.the-scientist.com/atom/latest', 'The Scientist', 'News'),
        ('https://www.science.org/rss/news_current.xml', 'Science', 'News'),
        ('https://www.nature.com/nature/rss/newsandcomment', 'Nature (News & Comment)', 'News'),

        # Bio industry news
        ('https://www.statnews.com/feed/', 'STAT News', 'News'),
        ('https://arstechnica.com/science/feed/', 'Ars Technica', 'News'),
        ('https://www.wired.com/feed/category/science/latest/rss', 'Wired', 'News'),
        ('https://www.fiercebiotech.com/rss/xml', 'Fierce Biotech', 'News'),
        ('https://endpts.com/feed/', 'Endpoints News', 'News'),
        ('https://www.biopharmadive.com/feeds/news/', 'BioPharmaDive', 'News'),
        ('https://www.clinicaltrialsarena.com/feed/', 'Clinical Trials Arena', 'News'),

        # Neuroscience
        ('https://www.thetransmitter.org/feed/', 'The Transmitter', 'Neuroscience'),


        # Research papers
        ('https://www.science.org/action/showFeed?type=etoc&feed=rss&jc=science', 'Science (Paper)', 'Paper'),
        ('https://www.cell.com/cell/current.rss', 'Cell', 'Paper'),
        ('https://www.nature.com/neuro/current_issue/rss', 'Nature Neuroscience', 'Paper'),
        ('https://www.nature.com/nm/current_issue/rss', 'Nature Medicine', 'Paper'),
        ('https://www.nature.com/nrd/current_issue/rss', 'Nature Drug Discovery', 'Paper'),
        ('https://www.nature.com/nbt/current_issue/rss', 'Nature Biotechnology', 'Paper'),
        ('https://www.nature.com/nature/research-articles.rss', 'Nature (Paper)', 'Paper'),
        ('https://www.nejm.org/action/showFeed?jc=nejm&type=etoc&feed=rss', 'NEJM', 'Paper')
    ]

    # 3. í¬ë¡¤ë§ ë° í›„ë³´ ì„ ì •
    candidates = []
    
    # 3-1. ì‹¤íŒ¨í–ˆë˜ ê²ƒ ìš°ì„  ì¶”ê°€
    if failed_queue:
        log(f"ì§€ë‚œ ì‹¤í–‰ ì‹¤íŒ¨ í•­ëª© {len(failed_queue)}ê°œ ì¬ì‹œë„ ëŒ€ê¸°", "INFO")
        for item in failed_queue:
            if item['url'] not in seen_urls:
                candidates.append(item)

    # ìœ íŠœë¸Œ ì±„ë„
    candidates.extend(
        scrape_youtube_videos('UCWgXoKQ4rl7SY9UHuAwxvzQ', 'B_ZCF YouTube', 'Video'))
    candidates.extend(
        scrape_youtube_videos('UCXql5C57vS4ogUt6CPEWWHA', 'ê¹€ì§€ìœ¤ì˜ ì§€ì‹Play YouTube', 'Video'))    
    
    candidates = [item for item in candidates if item['url'] not in seen_urls]
        
    
    # 3-2. ì‹ ê·œ í¬ë¡¤ë§
    for url, source, cat in sources:
        items = scrape_feed(url, source, cat)
        for item in items:
            if item['url'] not in seen_urls:
                candidates.append(item)
                
    

    # ì¤‘ë³µ ì œê±°
    unique_candidates = {v['url']: v for v in candidates}.values()
    log(f"ì´ ì²˜ë¦¬ ëŒ€ìƒ: {len(unique_candidates)}ê±´", "INFO")

    # 4. AI ì²˜ë¦¬
    new_articles = []
    new_failed_queue = []
    processed_cnt = 0
   
    for art in unique_candidates:
        if processed_cnt >= MAX_NEW_ARTICLES_PER_RUN:
            log(f"í• ë‹¹ëŸ‰({MAX_NEW_ARTICLES_PER_RUN}) ì´ˆê³¼. ë‚¨ì€ {len(unique_candidates) - processed_cnt}ê±´ì€ ë‹¤ìŒìœ¼ë¡œ ë¯¸ë£¸.", "WARNING")
            new_failed_queue.append(art)
            continue

        processed_cnt += 1
        
        model_id = 'gemini-2.5-flash-lite'
        title_kr, summary_kr = get_gemini_summary(art, model_id)
        
        if "[ìš”ì•½ ì‹¤íŒ¨]" in summary_kr:
            # ì‹¤íŒ¨ì‹œ íì— ì €ì¥ (ë‹¤ìŒ ì‹¤í–‰ë•Œ ìµœìš°ì„  ì²˜ë¦¬)
            new_failed_queue.append(art)
        else:
            art['title'] = title_kr
            art['summary_kr'] = summary_kr
            # ì›ë¬¸ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ (ìš©ëŸ‰ ì ˆì•½) or í•„ìš”ì‹œ art['summary_en'] = ...
            if 'description_en' in art: del art['description_en']
            new_articles.append(art)
        
        time.sleep(API_DELAY_SECONDS)

    # 5. ê²°ê³¼ ì €ì¥ (ì´ ë¶€ë¶„ì´ ê°€ì¥ ì¤‘ìš”. ì—ëŸ¬ê°€ ë‚˜ë„ ë°˜ë“œì‹œ ì €ì¥ë˜ë„ë¡ try-finallyë‚˜ ì•ˆì „ì¥ì¹˜ í•„ìš”)
    log(f"ì˜¤ëŠ˜ ì²˜ë¦¬ ê²°ê³¼: ì„±ê³µ {len(new_articles)}ê±´, ì‹¤íŒ¨/ë³´ë¥˜ {len(new_failed_queue)}ê±´", "INFO")

    final_list = old_articles + new_articles
    # ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    final_list.sort(key=lambda x: x.get('date', ''), reverse=True)

    output_data = {
        'last_updated': datetime.now(PALO_ALTO_TZ).strftime('%Y-%m-%d %H:%M:%S'),
        'failed_queue': new_failed_queue,
        'articles': final_list
    }

    try:
        with open('articles.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        log("ë°ì´í„° ì €ì¥ ì™„ë£Œ (articles.json)", "INFO")
    except Exception as e:
        log(f"ì¹˜ëª…ì  ì˜¤ë¥˜: íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ - {e}", "ERROR")
        # ì €ì¥ ì‹¤íŒ¨ì‹œ ë¡œê·¸ë¼ë„ ì¶œë ¥
        print(json.dumps(execution_logs, indent=2))
        sys.exit(1)

    # 6. logs.json ë³„ë„ ì €ì¥ (ë‚ ì§œë³„ ëˆ„ì )
    log_file_path = 'logs.json'
    all_logs = {}
    
    # ì˜¤ëŠ˜ ë‚ ì§œ í‚¤ ìƒì„± (Palo Alto ì‹œê°„ ê¸°ì¤€)
    current_date_key = start_time.strftime('%Y-%m-%d')

    if os.path.exists(log_file_path):
        try:
            with open('logs.json', 'r', encoding='utf-8') as f:
                all_logs = json.load(f)
                # ë§Œì•½ íŒŒì¼ ë‚´ìš©ì´ dictê°€ ì•„ë‹ˆë©´ ì´ˆê¸°í™”
                if not isinstance(all_logs, dict):
                    all_logs = {}
        except Exception as e:
            print(f"ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œë¨): {e}")
            all_logs = {}
            
    # ì˜¤ëŠ˜ ë‚ ì§œ í‚¤ì— í˜„ì¬ê¹Œì§€ ìŒ“ì¸ ë¡œê·¸(execution_logs) ì €ì¥
    all_logs[current_date_key] = execution_logs

    try:
        with open('logs.json', 'w', encoding='utf-8') as f:
            json.dump(all_logs, f, ensure_ascii=False, indent=2)
        print(f"ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_file_path} (Key: {current_date_key})")
    except Exception as e:
        print(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    print("=== ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ===")

if __name__ == '__main__':
    main()
