#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
[ì˜êµ¬ ë²„ì „ - GitHub Actionsìš©]
ì§€ì •ëœ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤/ë…¼ë¬¸ì„ í¬ë¡¤ë§í•˜ê³ 
Gemini APIë¥¼ ì´ìš©í•´ ë²ˆì—­/ìš”ì•½í•œ í›„
articles.jsonì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. (í•˜ë£¨ 50ê°œ ì œí•œ)
YouTubeëŠ” update_youtube_locally.pyë¡œ ë³„ë„ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import feedparser
import time
import os
import google.generativeai as genai
import google.generativeai.types as genai_types
from urllib.parse import urljoin
import sys

# --- ì„¤ì • ---
# í•˜ë£¨ì— ìš”ì•½í•  ìƒˆ ê¸°ì‚¬/ë…¼ë¬¸ì˜ ìµœëŒ€ ê°œìˆ˜ (API í• ë‹¹ëŸ‰ ë³´í˜¸)
MAX_NEW_ARTICLES_PER_RUN = 50

# --- AI ìš”ì•½ ê¸°ëŠ¥ ---

def get_gemini_summary(title_en, description_en):
    """
    Gemini APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì œëª©ê³¼ ì„¤ëª…ì„ í•œê¸€ë¡œ ë²ˆì—­ ë° ìš”ì•½í•©ë‹ˆë‹¤.
    """
    print(f"  [AI] '{title_en[:30]}...' ë²ˆì—­/ìš”ì•½ ìš”ì²­ ì¤‘...")
    
    try:
        api_key = os.environ.get('GEMINI_API_KEY')
        
        if not api_key:
            print("  [AI] âŒ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ì›ë³¸: {description_en[:100]}...)"

        genai.configure(api_key=api_key)
        
        generation_config = genai.GenerationConfig(response_mime_type="application/json")
        model = genai.GenerativeModel(
            'gemini-2.5-flash-preview-09-2025',
            generation_config=generation_config
        )
        
        prompt = f"""
        ë‹¹ì‹ ì€ ì „ë¬¸ ê³¼í•™ ë‰´ìŠ¤ í¸ì§‘ìì…ë‹ˆë‹¤.
        ì•„ë˜ì˜ ì˜ì–´ ê¸°ì‚¬ ì œëª©ê³¼ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ, í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
        ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

        [ì…ë ¥]
        - title_en: "{title_en}"
        - description_en: "{description_en}"

        [JSON ì¶œë ¥ í˜•ì‹]
        {{
          "title_kr": "ì—¬ê¸°ì— í•œêµ­ì–´ ë²ˆì—­ ì œëª©ì„ ì‘ì„±",
          "summary_kr": "ì—¬ê¸°ì— 5-6 ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ìƒì„¸í•œ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±"
        }}

        [ê·œì¹™]
        1. "title_kr" í‚¤ì—ëŠ” "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
        2. "summary_kr" í‚¤ì—ëŠ” "description_en"ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ 5-6 ë¬¸ì¥ì˜ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
        3. ì¹œì ˆí•œ ë§íˆ¬ê°€ ì•„ë‹Œ, ì „ë¬¸ì ì´ê³  ê°„ê²°í•œ ë‰´ìŠ¤ì²´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        """
        
        # API í˜¸ì¶œ ì‹œ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        response = model.generate_content(prompt, request_options={'timeout': 120})
        
        data = json.loads(response.text)
        
        title_kr = data.get('title_kr', title_en)
        summary_kr = data.get('summary_kr', f"[ìš”ì•½ ì‹¤íŒ¨] API ì‘ë‹µ ì˜¤ë¥˜. (ì›ë³¸: {description_en[:100]}...)")
        
        print(f"  [AI] âœ“ ìš”ì•½ ì™„ë£Œ: {title_kr[:30]}...")
        return title_kr, summary_kr
    
    except Exception as e:
        # 'BlockedPromptError'ê°€ ì•„ë‹Œ 'BlockedPromptException'ìœ¼ë¡œ ìˆ˜ì •
        if isinstance(e, genai_types.generation_types.BlockedPromptException):
             print(f"  [AI] âŒ Gemini API - ì½˜í…ì¸  ì°¨ë‹¨ ì˜¤ë¥˜: {e}")
             return title_en, "[ìš”ì•½ ì‹¤íŒ¨] APIê°€ ì½˜í…ì¸ ë¥¼ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤."
        
        print(f"  [AI] âŒ Gemini API ì˜¤ë¥˜: {e}")
        # API í• ë‹¹ëŸ‰ ì´ˆê³¼(ResourceExhausted) ë“±ì˜ ì˜¤ë¥˜ í¬í•¨
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. (ì›ë³¸: {description_en[:100]}...)"
    
    except json.JSONDecodeError as e:
        print(f"  [AI] âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}. ì‘ë‹µ í…ìŠ¤íŠ¸: {response.text[:100]}...")
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜. (ì›ë³¸: {description_en[:100]}...)"


# --- ì›¹ì‚¬ì´íŠ¸ë³„ ìŠ¤í¬ë˜í¼ ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
    'Accept': 'application/xml,application/rss+xml,text/xml;q=0.9,text/html;q=0.8,*/*;q=0.5',
    'Accept-Language': 'en-US,en;q=0.9,ko;q=0.8',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
}

# [ì‹ ê·œ] ë´‡ ì°¨ë‹¨ ë° ì˜¤ë¥˜ì— ê°•í•œ RSS í”¼ë“œ íŒŒì‹± í•¨ìˆ˜
def scrape_robust_rss_feed(feed_url, source_name, category_name):
    """
    requestsë¡œ ë¨¼ì € ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¨ í›„ feedparserë¡œ íŒŒì‹±í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì¸ í•¨ìˆ˜.
    """
    articles = []
    print(f"ğŸ” [{source_name}] (RSS) í¬ë¡¤ë§ ì¤‘... (URL: {feed_url})")
    
    try:
        # requestsë¡œ ë¨¼ì € ì ‘ì† ì‹œë„
        response = requests.get(feed_url, headers=HEADERS, timeout=20)
        
        # HTTP ì˜¤ë¥˜ í™•ì¸ (ì˜ˆ: 404, 500)
        response.raise_for_status() 
        
        # Content-Type í™•ì¸ (XML/RSSê°€ ë§ëŠ”ì§€)
        content_type = response.headers.get('Content-Type', '').lower()
        if 'xml' not in content_type and 'rss' not in content_type:
            print(f"  âŒ RSS í”¼ë“œê°€ XML/RSSê°€ ì•„ë‹Œ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. (Content-Type: {content_type})")
            print(f"     ì‘ë‹µ ë‚´ìš© (ì²« 200ì): {response.text[:200]}...")
            return [] # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        # requestsë¡œ ê°€ì ¸ì˜¨ ì½˜í…ì¸ ë¥¼ feedparserë¡œ íŒŒì‹±
        feed = feedparser.parse(response.content)
        
        # feedparser íŒŒì‹± ì˜¤ë¥˜ í™•ì¸ (bozo í”Œë˜ê·¸)
        if feed.bozo:
            print(f"  âš ï¸ RSS í”¼ë“œ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (bozo): {feed.bozo_exception}")
            # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ìµœëŒ€í•œ íŒŒì‹±ëœ í•­ëª©ì€ ì²˜ë¦¬ ì‹œë„
        
        print(f"  [i] {len(feed.entries)}ê°œ í•­ëª© ì°¾ìŒ")

        for entry in feed.entries:
            try:
                title_en = entry.title
                link = entry.link
                
                # ì„¤ëª…: summary > description > title ìˆœì„œë¡œ ì°¾ê¸°
                description_en = entry.summary if hasattr(entry, 'summary') else (entry.description if hasattr(entry, 'description') else title_en)
                
                # HTML íƒœê·¸ ì œê±°
                description_text = BeautifulSoup(description_en, 'html.parser').get_text(strip=True)
                
                # ë‚ ì§œ íŒŒì‹± (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ëŒ€ì²´)
                date_str = datetime.now().strftime('%Y-%m-%d')
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    try:
                        # struct_timeì„ datetime ê°ì²´ë¡œ ë³€í™˜ í›„ í¬ë§·íŒ…
                        dt_obj = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                        date_str = dt_obj.strftime('%Y-%m-%d')
                    except (TypeError, ValueError) as date_err:
                        print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {date_err}, ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©.")
                
                # ì´ë¯¸ì§€ ì¶”ì¶œ (media_thumbnail, enclosure, description ë‚´ img íƒœê·¸ ìˆœ)
                image_url = None
                if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0].get('url')
                elif hasattr(entry, 'links'):
                    for e_link in entry.links:
                        if e_link.get('rel') == 'enclosure' and e_link.get('type', '').startswith('image/'):
                            image_url = e_link.get('href')
                            break
                if not image_url and description_en: # ì„¤ëª… í•„ë“œì— HTMLì´ ìˆì„ ê²½ìš°
                    desc_soup = BeautifulSoup(description_en, 'html.parser')
                    img_tag = desc_soup.find('img')
                    if img_tag:
                        # ìƒëŒ€ URLì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ˆëŒ€ URLë¡œ ë³€í™˜ ì‹œë„
                        img_src = img_tag.get('src')
                        if img_src:
                            image_url = urljoin(link, img_src) # ê¸°ì‚¬ ë§í¬ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ URL ìƒì„±
                            
                # ì œëª©ì—ì„œ HTML íƒœê·¸ ì œê±° (<Emphasis> ë“±)
                title_en = BeautifulSoup(title_en, 'html.parser').get_text(strip=True)

                articles.append({
                    'title_en': title_en,
                    'description_en': description_text,
                    'url': link,
                    'source': source_name,
                    'category': category_name,
                    'date': date_str,
                    'image_url': image_url
                })
                
            except Exception as item_err:
                # ê°œë³„ í•­ëª© íŒŒì‹± ì˜¤ë¥˜ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
                print(f"  âœ— RSS ê°œë³„ í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {item_err}")
            
    except requests.exceptions.RequestException as req_err:
        # requests ê´€ë ¨ ì˜¤ë¥˜ (ì—°ê²° ì‹¤íŒ¨, íƒ€ì„ì•„ì›ƒ, HTTP ì˜¤ë¥˜ ë“±)
        print(f"âŒ [{source_name}] RSS ìš”ì²­ ì‹¤íŒ¨: {req_err}")
    except Exception as e:
        # ê·¸ ì™¸ ì˜ˆê¸°ì¹˜ ëª»í•œ ì „ì²´ ì˜¤ë¥˜
        print(f"âŒ [{source_name}] RSS í¬ë¡¤ë§ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    
    return articles


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ“° ì¼ì¼ ì½ì„ê±°ë¦¬ ìë™ ìˆ˜ì§‘ ë° ìš”ì•½ ì‹œì‘")
    print(f"ğŸ• ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60 + "\n")
    
    all_articles_to_check = []
    
    # --- [ìˆ˜ì •] ë‹˜ì´ ì£¼ì‹  RSS í”¼ë“œ ëª©ë¡ ì „ì²´ (scrape_robust_rss_feed ì‚¬ìš©) ---
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.nature.com/nature/rss/articles?type=news', 'Nature', 'News'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.science.org/rss/news_current.xml', 'Science', 'News'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.thetransmitter.org/feed/', 'The Transmitter', 'Neuroscience'))
    
    
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.science.org/action/showFeed?type=etoc&feed=rss&jc=science', 'Science (Paper)', 'Paper'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.cell.com/cell/current.rss', 'Cell', 'Paper'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.nature.com/neuro/current_issue/rss', 'Nature Neuroscience', 'Paper'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.nature.com/nm/current_issue/rss', 'Nature Medicine', 'Paper'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.nature.com/nrd/current_issue/rss', 'Nature Drug Discovery', 'Paper'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.nature.com/nbt/current_issue/rss', 'Nature Biotechnology', 'Paper'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.nature.com/nature/rss/newsandcomment', 'Nature (News & Comment)', 'News'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.nature.com/nature/research-articles.rss', 'Nature (Paper)', 'Paper'))
    
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.statnews.com/feed/', 'STAT News', 'News'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.the-scientist.com/rss', 'The Scientist', 'News'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://arstechnica.com/science/feed/', 'Ars Technica', 'News'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.wired.com/feed/category/science/latest/rss', 'Wired', 'News'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://neurosciencenews.com/feed/', 'Neuroscience News', 'News'))
    
    # [ìˆ˜ì •] FDA ì£¼ì†Œ ë³€ê²½
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.fda.gov/about-fda/contact-fda/stay-informed/rss-feeds/drugs/rss.xml', 'FDA', 'News')) # Drugs í”¼ë“œëŠ” ì´ í˜ì´ì§€ ë‚´ì—ì„œ ì°¾ì•„ì•¼ í•  ìˆ˜ ìˆìŒ
    
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.fiercebiotech.com/rss/xml', 'Fierce Biotech', 'News'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://endpts.com/feed/', 'Endpoints News', 'News'))
    all_articles_to_check.extend(scrape_robust_rss_feed('https://www.nejm.org/action/showFeed?jc=nejm&type=etoc&feed=rss', 'NEJM', 'Paper'))
    
    # [ìˆ˜ì •] JAMA ì£¼ì†Œ ë³€ê²½ (ì‚¬ì´íŠ¸ í”¼ë“œ í˜ì´ì§€ ì°¸ê³ )
    all_articles_to_check.extend(scrape_robust_rss_feed('https://jamanetwork.com/rss/latest.xml', 'JAMA', 'Paper')) # 'ìµœì‹  ì „ì²´' í”¼ë“œ
    
    
    seen_urls = set()
    final_article_list = [] # ìµœì¢… ì €ì¥ë  ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ + ì‹ ê·œ)
    
    try:
        with open('articles.json', 'r', encoding='utf-8') as f:
            old_data = json.load(f)
            # ìµœê·¼ 7ì¼ê°„ì˜ ê¸°ì‚¬ë§Œ URL ì²´í¬ ë° ìµœì¢… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            for old_article in old_data.get('articles', []):
                try:
                    article_date = datetime.strptime(old_article.get('date', '1970-01-01'), '%Y-%m-%d')
                    if (datetime.now() - article_date).days <= 7:
                        if old_article.get('url'):
                            seen_urls.add(old_article['url'])
                            final_article_list.append(old_article)
                except ValueError:
                    continue # ë‚ ì§œ í˜•ì‹ì´ ë‹¤ë¥´ë©´ ë¬´ì‹œ
        print(f"\n[i] ê¸°ì¡´ {len(seen_urls)}ê°œì˜ URL (ìµœê·¼ 7ì¼)ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ê¸°ì‚¬ë§Œ ì¶”ê°€/ìš”ì•½í•©ë‹ˆë‹¤.")
    except FileNotFoundError:
        print("\n[i] 'articles.json' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    except json.JSONDecodeError:
        print("\n[i] âŒ 'articles.json' íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        final_article_list = []
        seen_urls = set()
    
    
    new_articles = [] # ìƒˆë¡œ ìš”ì•½ëœ ê¸°ì‚¬ë§Œ ì„ì‹œ ë³´ê´€
    existing_articles_count = 0
    new_article_count = 0
    api_errors = 0 # API ì˜¤ë¥˜ íšŸìˆ˜
    
    print(f"\n[i] ì´ {len(all_articles_to_check)}ê°œì˜ (RSS) í•­ëª©ì„ í™•ì¸í•©ë‹ˆë‹¤ (ìµœëŒ€ {MAX_NEW_ARTICLES_PER_RUN}ê°œê¹Œì§€ ìš”ì•½).")

    for article_data in all_articles_to_check:
        
        # URL ëˆ„ë½ ë˜ëŠ” ë¹ˆ URL ì²´í¬
        if not article_data.get('url'):
            print(f"  âš ï¸ URLì´ ì—†ëŠ” í•­ëª© ë°œê²¬ (Source: {article_data.get('source', 'N/A')}). ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        if article_data['url'] not in seen_urls:
            
            if new_article_count >= MAX_NEW_ARTICLES_PER_RUN:
                print(f"  [i] API í• ë‹¹ëŸ‰ ë³´í˜¸ë¥¼ ìœ„í•´ {MAX_NEW_ARTICLES_PER_RUN}ê°œ ë„ë‹¬. ë‚˜ë¨¸ì§€ëŠ” ë‹¤ìŒ ì‹¤í–‰ìœ¼ë¡œ...")
                break # í•˜ë£¨ ìµœëŒ€ì¹˜ì— ë„ë‹¬í•˜ë©´ ë£¨í”„ ì¤‘ë‹¨
            
            new_article_count += 1
            print(f"  [i] âœ¨ ìƒˆë¡œìš´ ê¸°ì‚¬ ë°œê²¬ ({new_article_count}/{MAX_NEW_ARTICLES_PER_RUN}): {article_data['title_en'][:50]}...")
            
            # API í˜¸ì¶œë¡œ ë²ˆì—­ ë° ìš”ì•½
            title_kr, summary_kr = get_gemini_summary(article_data['title_en'], article_data['description_en'])
            
            # API ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ì¹´ìš´íŠ¸ ì¦ê°€
            if "[ìš”ì•½ ì‹¤íŒ¨]" in summary_kr:
                api_errors += 1
                
            article_data['title'] = title_kr
            article_data['summary_kr'] = summary_kr
            
            # ì›ë³¸ ì˜ì–´ ì œëª©/ì„¤ëª…ë„ ì €ì¥
            article_data['title_en'] = article_data['title_en']
            article_data['summary_en'] = article_data['description_en']
            del article_data['description_en'] # ì¤‘ë³µ í•„ë“œ ì œê±°
            
            new_articles.append(article_data)
            seen_urls.add(article_data['url'])
            
            time.sleep(1) # API ë”œë ˆì´
            
        elif article_data.get('url'):
            existing_articles_count += 1
    
    print(f"\n[i] {new_article_count}ê°œì˜ ìƒˆë¡œìš´ (RSS) ê¸°ì‚¬ë¥¼ ìš”ì•½ ì‹œë„í–ˆìŠµë‹ˆë‹¤.")
    print(f"    (ì„±ê³µ: {new_article_count - api_errors}ê°œ, API ì˜¤ë¥˜: {api_errors}ê°œ)")
    print(f"    (ì¤‘ë³µ/ê¸°ì¡´ ê¸°ì‚¬ {existing_articles_count}ê°œ ì œì™¸)")
    
    
    # 3. ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í•©ì¹¨
    final_article_list.extend(new_articles)
    
    # 4. í•©ì¹œ ëª©ë¡ì—ì„œ ë‹¤ì‹œ ì¤‘ë³µ ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš° ëŒ€ë¹„)
    final_seen_urls = set()
    deduplicated_list = []
    for article in final_article_list:
        if article.get('url') not in final_seen_urls:
            if article.get('url'):
                final_seen_urls.add(article['url'])
                deduplicated_list.append(article)

    # 5. ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    deduplicated_list.sort(key=lambda x: x.get('date', '1970-01-01'), reverse=True)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'articles': deduplicated_list
    }
    
    json_file_path = 'articles.json'
    try:
        with open(json_file_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ì™„ë£Œ! ì´ {len(deduplicated_list)}ê°œ í•­ëª© ì €ì¥ (ìµœê·¼ 7ì¼ + ì‹ ê·œ)")
        print(f"ğŸ“ '{json_file_path}' íŒŒì¼ ì—…ë°ì´íŠ¸ë¨")
    except Exception as write_err:
        print(f"\nâŒ JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {write_err}")
        sys.exit(1) # ì˜¤ë¥˜ ì½”ë“œì™€ í•¨ê»˜ ì¢…ë£Œ

    print("\n" + "="*60 + "\n")
    
    sources = {}
    for article in deduplicated_list:
        source = article.get('source', 'Unknown')
        sources[source] = sources.get(source, 0) + 1
    
    print("ğŸ“Š ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ í˜„í™© (ìµœê·¼ 7ì¼ + ì‹ ê·œ):")
    for source, count in sorted(sources.items()):
        print(f"  â€¢ {source}: {count}ê°œ")


if __name__ == '__main__':
    main()
