#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¼ì¼ ê³¼í•™ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ìµœì¢… ìˆ˜ì •íŒ)
- ZoneInfoë¥¼ í†µí•œ ì •í™•í•œ Palo Alto ì‹œê°„ ì ìš©
- AI ì‘ë‹µ(List/Dict) ìœ ì—°í•œ ì²˜ë¦¬ (ì—ëŸ¬ ë°©ì§€)
- API ë° ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ì— ëŒ€í•œ ê°•í•œ ë‚´ì„± ë° ë¡œê¹…
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import feedparser
import time
import os
from google import genai
from google.genai import types
import sys
import re
# íƒ€ì„ì¡´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Python 3.9+)
try:
    from zoneinfo import ZoneInfo
except ImportError:
    # êµ¬ë²„ì „ íŒŒì´ì¬ ëŒ€ë¹„ (GitHub ActionsëŠ” ë³´í†µ ìµœì‹ ì„)
    from datetime import timezone, timedelta
    class ZoneInfo:
        def __init__(self, key): pass
        def utcoffset(self, dt): return timedelta(hours=-8)
        def tzname(self, dt): return "PST"
        def dst(self, dt): return timedelta(0)

# ============================================================================
# ì„¤ì •
# ============================================================================

MAX_NEW_ARTICLES_PER_RUN = 8000
API_DELAY_SECONDS = 2 # API ì•ˆì •ì„±ì„ ìœ„í•´ 1ì´ˆ -> 2ì´ˆë¡œ ëŠ˜ë¦¼
# ì¬ì‹œë„ íšŸìˆ˜ ì„¤ì • (ì´ 3ë²ˆ ì‹œë„)
max_retries = 5

# íŒ”ë¡œì•Œí†  ì‹œê°„ëŒ€ (ì¸ë¨¸íƒ€ì„ ìë™ ì ìš©)
try:
    PALO_ALTO_TZ = ZoneInfo("America/Los_Angeles")
except:
    PALO_ALTO_TZ = timezone(timedelta(hours=-8)) # fallback

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'application/xml,application/rss+xml,text/xml;q=0.9,*/*;q=0.5'
}

# ì‹¤í–‰ ë¡œê·¸ ì „ì—­ ë³€ìˆ˜
execution_logs = []

def log(message, level="INFO"):
    """ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤."""
    now = datetime.now(PALO_ALTO_TZ)
    timestamp = now.strftime('%H:%M:%S')
    
    # ì½˜ì†” ì¶œë ¥ (GitHub Actions ë¡œê·¸ìš©)
    print(f"[{timestamp}] [{level}] {message}")
    
    # íŒŒì¼ ì €ì¥ìš© ë¡œê·¸ (ì›¹ í‘œì‹œìš©)
    execution_logs.append({
        "time": timestamp,
        "level": level,
        "message": message
    })

# ============================================================================
# AI ë²ˆì—­ ë° ìš”ì•½
# ============================================================================

def clean_json_text(text):
    """JSON ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ì œ"""
    text = re.sub(r'^```json\s*', '', text)
    text = re.sub(r'^```\s*', '', text)
    text = re.sub(r'\s*```$', '', text)
    return text.strip()

def get_gemini_batch_summary(articles_batch):
    
    api_key = os.environ.get('GEMINI_API_KEY')
    
    if not api_key:
        log("API Key ëˆ„ë½", "ERROR")
        return []

    client = genai.Client(api_key=api_key)
    
    # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_intro = """

    ë‹¹ì‹ ì€ ì „ë¬¸ ê³¼í•™ ê¸°ìì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µë˜ëŠ” ê³¼í•™ ê¸°ì‚¬ë“¤ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  ìš”ì•½í•˜ì„¸ìš”.
    
    ë‹¹ì‹ ì€ ê³¼í•™ì— ëŠ¥í†µí•œ ì „ë¬¸ ê¸°ì í˜¹ì€ ì»¤ë®¤ë‹ˆì¼€ì´í„°ì…ë‹ˆë‹¤.
    ì•„ì•„ë˜ ì œê³µë˜ëŠ” ê³¼í•™ ê¸°ì‚¬ë“¤ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±í•˜ì„¸ìš”.
    ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

    
    [í•„ìˆ˜ ê·œì¹™]
    1. ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ JSON í¬ë§·ì„ ì •í™•íˆ ì¤€ìˆ˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    2. 'id'ëŠ” ì…ë ¥ëœ ê¸°ì‚¬ì˜ ìˆœì„œ ë²ˆí˜¸ì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
    3. 'title_kr': ì „ë¬¸ì ì¸ í•œêµ­ì–´ ì œëª©.
    4. "title_kr" í‚¤ì—ëŠ” "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
    5. 'summary_kr': ì—¬ê¸°ì— ìµœì†Œ 5-6 ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ìƒì„¸í•œ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±
    6. "summary_kr" í‚¤ì—ëŠ” "description_en"ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
    7. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì²´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
    
    [ì…ë ¥ ë°ì´í„°]
    """
    
    articles_text = ""
    for idx, art in enumerate(articles_batch):
        articles_text += f"""
        ---
        ID: {idx}
        Title: {art['title_en']}
        Description: {art['description_en']}
        ---
        """

    prompt_full = prompt_intro + articles_text

    # 2. API í˜¸ì¶œ
    for attempt in range(5): # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ 5ë²ˆê¹Œì§€ ì¬ì‹œë„
        try:
            response = client.models.generate_content(
                model='gemini-2.5-flash-lite',
                contents=prompt_full,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema={
                        "type": "ARRAY",
                        "items": {
                            "type": "OBJECT",
                            "properties": {
                                "id": {"type": "INTEGER"},
                                "title_kr": {"type": "STRING"},
                                "summary_kr": {"type": "STRING"}
                            }
                        }
                    }
                )
            )
            
            # 3. ê²°ê³¼ íŒŒì‹±
            results = json.loads(response.text)
            
            # ê²°ê³¼ ë§¤í•‘ (ID ê¸°ì¤€ìœ¼ë¡œ ì›ë˜ ê¸°ì‚¬ì— ë§¤ì¹­)
            processed_batch = []
            result_map = {item['id']: item for item in results}
            
            for idx, art in enumerate(articles_batch):
                if idx in result_map:
                    art['title'] = result_map[idx]['title_kr']
                    art['summary_kr'] = result_map[idx]['summary_kr']
                    if 'description_en' in art: del art['description_en'] # ìš©ëŸ‰ ì ˆì•½
                    processed_batch.append(art)
                else:
                    # AIê°€ íŠ¹ì • ê¸°ì‚¬ë¥¼ ë¹¼ë¨¹ì—ˆì„ ê²½ìš° ì›ë³¸ ìœ ì§€ í›„ ì‹¤íŒ¨ ì²˜ë¦¬ ë¡œì§ ë“±ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŒ
                    log(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ëˆ„ë½ë¨: {art['title_en'][:10]}...", "WARNING")
                    art['title'] = art['title_en']
                    art['summary_kr'] = "[ìš”ì•½ ì‹¤íŒ¨] ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ëˆ„ë½"
                    processed_batch.append(art)

            log(f"  âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ! ({len(processed_batch)}ê°œ ê¸°ì‚¬ ìš”ì•½ë¨)", "INFO")
            return processed_batch

        except Exception as e:
            wait = 120
            log(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬(ì‹œë„ {attempt+1}): {e}. {wait}ì´ˆ ëŒ€ê¸°...", "WARNING")
            time.sleep(wait)
    
    # ìµœì¢… ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    return articles_batch


def get_gemini_summary_youtube(article_data):
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ì½˜í…ì¸ ë¥¼ ë²ˆì—­í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.
    ìœ íŠœë¸Œ ì˜ìƒì˜ ê²½ìš° URLì„ í†µí•´ ì§ì ‘ ì˜ìƒ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        article_data (dict): title_en, description_en, url, sourceë¥¼ í¬í•¨í•œ ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°
        
    Returns:
        tuple: (translated_title_kr, summary_kr)
    """
    title_en = article_data['title_en']
    description_en = article_data['description_en']
    url = article_data['url']
    source = article_data.get('source', '')

    api_key = os.environ.get('GEMINI_API_KEY') 
        
    if not api_key:
        print("  [AI] âŒ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í‚¤ ì—†ìŒ. (ì›ë³¸: {description_en[:100]}...)"
    
    client = genai.Client(api_key=api_key)

    for attempt in range(max_retries):
        try:        
            # ìœ íŠœë¸Œ ì˜ìƒ: URLì„ í†µí•´ ì§ì ‘ ì˜ìƒ ì½˜í…ì¸  ë¶„ì„
            if 'YouTube' in source:
                print(f"  [AI] ğŸ¥ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ ì¤‘: '{title_en[:40]}...'")
                
                prompt = f"""
                        ë‹¹ì‹ ì€ ì˜ìƒ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ ìœ íŠœë¸Œ ì˜ìƒì„ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.
                        ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
                        
                        [ì…ë ¥]
                        - title_en: "{title_en}"
                        
                        [JSON ì¶œë ¥ í˜•ì‹]
                        {{
                          "title_kr": "ì—¬ê¸°ì— ì œëª©ì˜ ì „ë¬¸ì ì¸ í•œêµ­ì–´ ë²ˆì—­ì„ ì‘ì„±í•©ë‹ˆë‹¤",
                          "summary_kr": "í•µì‹¬ ìš”ì ì„ ì¶”ì¶œí•˜ì—¬, ì˜ìƒ ì½˜í…ì¸ ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ìµœì†Œ 10ë¬¸ì¥ ë¶„ëŸ‰ì˜ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤"
                        }}
                        
                        [ê·œì¹™]
                        1. "title_kr": "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
                        2. "summary_kr": ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì²´ë¡œ ìƒì„¸í•œ ìµœì†Œ 10ë¬¸ì¥ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
                        3. ëŒ€í™”ì²´ê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ê¸€ì“°ê¸° ë¬¸ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                        """
    
                response = client.models.generate_content(
                    model='gemini-2.5-flash-lite', # ëª¨ë¸ ë²„ì „
                    contents=[
                        prompt,
                        types.Part.from_uri(
                            file_uri=url,
                            mime_type="video/youtube"
                        )
                    ],
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json"
                    )
                )
                
            
    
            # [ìˆ˜ì •ëœ ë¶€ë¶„] í…ìŠ¤íŠ¸ ì •ì œ (ë§ˆí¬ë‹¤ìš´ ì œê±°)
            text = response.text
            if text.startswith("```"):
                text = re.sub(r"^```json\s*", "", text) # ì‹œì‘ ë¶€ë¶„ ```json ì œê±°
                text = re.sub(r"^```\s*", "", text)     # ì‹œì‘ ë¶€ë¶„ ``` ì œê±°
                text = re.sub(r"\s*```$", "", text)     # ë ë¶€ë¶„ ``` ì œê±°
            
            text = text.strip() # ì•ë’¤ ê³µë°± ì œê±°
    
            # JSON íŒŒì‹±
            data = json.loads(text, strict=False)
            
            title_kr = data.get('title_kr', title_en)
            summary_kr = data.get('summary_kr', "ìš”ì•½ ë‚´ìš© ì—†ìŒ")
    
            log(f"  [AI] âœ… ì™„ë£Œ: {title_kr[:20]}...")
            return title_kr, summary_kr
    
        except json.JSONDecodeError as e:
            print(f"  [AI] âŒ JSON íŒŒì‹± ì—ëŸ¬: {e}")
            print(f"  [ë””ë²„ê·¸] ë¬¸ì œì˜ í…ìŠ¤íŠ¸: {response.text[:100]}...") # ë””ë²„ê¹…ìš© ì¶œë ¥
            return title_en, "[ìš”ì•½ ì‹¤íŒ¨] AI ì‘ë‹µ ì˜¤ë¥˜ (JSON íŒŒì‹± ì‹¤íŒ¨)"
        
        except Exception as e:
            
            wait_time = 120
            print(f"  [AI] âš ï¸ ì—ëŸ¬ ë°œìƒ (ì‹œë„ {attempt+1}): {e}")
            
            if attempt < max_retries - 1:
                print(f"  â³ {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(wait_time)
                continue
            else:
                print(f"  [AI] âŒ ìµœì¢… ì‹¤íŒ¨: {title_en[:20]}...")
                return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] {str(e)}"
                
# ============================================================================
# ìŠ¤í¬ë˜í¼
# ============================================================================

def scrape_feed(feed_url, source_name, category_name):
    articles = []
    log(f"í¬ë¡¤ë§ ì‹œì‘: {source_name}", "INFO")

    try:
        response = requests.get(feed_url, headers=HEADERS, timeout=10)
        feed = feedparser.parse(response.content)
        
        # [í•µì‹¬] ì´ ì‹œê°„ì´ ê³§ ê¸°ì‚¬ì˜ ë‚ ì§œê°€ ë©ë‹ˆë‹¤.
        palo_alto_now = datetime.now(PALO_ALTO_TZ)
        date_str = palo_alto_now.strftime('%Y-%m-%d')

        for entry in feed.entries:
            link = entry.get('link')
            title = entry.get('title')
            
            if not link or not title:
                continue

            # [ì‚­ì œë¨] ê¸°ì¡´ì˜ published_parsed ë¡œì§ ì œê±°
            # ë¬´ì¡°ê±´ ìœ„ì—ì„œ ì •ì˜í•œ date_str(ì˜¤ëŠ˜)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            image_url = None
            if entry.get('media_thumbnail'):
                image_url = entry.media_thumbnail[0]['url']
            elif entry.get('links'):
                for lk in entry.links:
                    if lk.get('type', '').startswith('image/'):
                        image_url = lk.get('href')
                        break
            
            # ë‚´ìš© ì¶”ì¶œ
            desc = entry.get('summary', '')
            clean_desc = BeautifulSoup(desc, 'html.parser').get_text(strip=True)

            articles.append({
                'title_en': title,
                'description_en': clean_desc,
                'url': link,
                'source': source_name,
                'category': category_name,
                'date': date_str, # ë¬´ì¡°ê±´ ì˜¤ëŠ˜ ë‚ ì§œ
                'image_url': image_url
            })
            
        log(f"[{source_name}] ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨", "INFO")

    except requests.exceptions.Timeout:
        log(f"{source_name}: ì—°ê²° ì‹œê°„ ì´ˆê³¼ (Timeout)", "ERROR")
    except Exception as e:
        log(f"{source_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")

    return articles
# ============================================================================
# ìœ íŠœë¸Œ ì±„ë„ ìŠ¤í¬ë˜í¼
# ============================================================================

def scrape_youtube_videos(channel_id, source_name, category_name):
    articles = []
    log(f"ğŸ” [{source_name}] ìœ íŠœë¸Œ í¬ë¡¤ë§ ì¤‘... (ì±„ë„: {channel_id})")
    feed_url = f'https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}'

    try:
        response = requests.get(feed_url, headers=HEADERS, timeout=20)
        response.raise_for_status()
        feed = feedparser.parse(response.content)

        # [í•µì‹¬] ìœ íŠœë¸Œë„ ë¬´ì¡°ê±´ ì˜¤ëŠ˜(Palo Alto) ë‚ ì§œë¡œ ê³ ì •
        palo_alto_now = datetime.now(PALO_ALTO_TZ)
        date_str = palo_alto_now.strftime('%Y-%m-%d')

        print(f"  [i] {len(feed.entries)}ê°œì˜ ìµœì‹  ì˜ìƒ ë°œê²¬")

        for entry in feed.entries:
            try:
                if not entry.get('title') or not entry.get('link'):
                    continue

                title_en = entry.title
                link = entry.link
                video_id = link.split('v=')[-1]

                # [ì‚­ì œë¨] ê¸°ì¡´ì˜ published_parsed ë¡œì§ ì œê±°
                
                # ê³ í™”ì§ˆ ì¸ë„¤ì¼
                image_url = None
                if entry.get('media_thumbnail') and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0]['url'].replace('default.jpg', 'hqdefault.jpg')

                description_en = entry.get('media_description', entry.get('summary', title_en))
                description_text = BeautifulSoup(description_en, 'html.parser').get_text(strip=True)
                
                log(f"    [i] ì˜ìƒ {video_id} ë¡œë“œë¨.")

                articles.append({
                    'title_en': title_en,
                    'description_en': description_text,
                    'url': link,
                    'source': source_name,
                    'category': category_name,
                    'date': date_str, # ë¬´ì¡°ê±´ ì˜¤ëŠ˜ ë‚ ì§œ
                    'image_url': image_url
                })

            except Exception as item_err:
                log(f"  âœ— ì˜ìƒ íŒŒì‹± ì‹¤íŒ¨: {item_err}")

    except Exception as e:
        log(f"âŒ [{source_name}] ì˜¤ë¥˜: {e}")

    return articles


def split_into_n_chunks(lst, n):
    """ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ê· ë“±í•˜ê²Œ nê°œì˜ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
    if not lst:
        return []
    # ë§Œì•½ ê¸°ì‚¬ ìˆ˜ê°€ n(19)ë³´ë‹¤ ì ìœ¼ë©´, ê¸°ì‚¬ ìˆ˜ë§Œí¼ë§Œ ë©ì–´ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    if len(lst) < n:
        return [[x] for x in lst]
        
    k, m = divmod(len(lst), n)
    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]

# %%
# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    start_time = datetime.now(PALO_ALTO_TZ)
    log(f"=== ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ (ë‚ ì§œ: {start_time.strftime('%Y-%m-%d')}) ===", "INFO")

    # 1. ë°ì´í„° ë¡œë“œ
    seen_urls = set()
    old_articles = []
    failed_queue = []

    try:
        with open('articles.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # [í•µì‹¬] ë‚ ì§œ ì œí•œ ì—†ì´ ëª¨ë“  ê³¼ê±° ê¸°ë¡ì„ ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ìš©)
            for art in data.get('articles', []):
                # URLì´ ìˆëŠ” ìœ íš¨í•œ ê¸°ì‚¬ë§Œ ë¡œë“œ
                if art.get('url'):
                    old_articles.append(art)
                    seen_urls.add(art['url'])
            
            # ì´ì „ ì‹¤íŒ¨ ëª©ë¡ ë¡œë“œ
            failed_queue = data.get('failed_queue', [])
            
    except FileNotFoundError:
        log("ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ì—†ìŒ. ìƒˆë¡œ ì‹œì‘.", "WARNING")
    except json.JSONDecodeError:
        log("JSON íŒŒì¼ ê¹¨ì§. ë°±ì—… í›„ ìƒˆë¡œ ì‹œì‘.", "ERROR")
    except Exception as e:
        log(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}", "ERROR")

    # 2. ìˆ˜ì§‘ ì†ŒìŠ¤ ì •ì˜
    sources = [
        # General
        ('https://www.technologyreview.com/feed/', 'MIT Tech Rev', 'News'),
        ('https://www.nature.com/nature/rss/articles?type=news', 'Nature', 'News'),
        ('https://www.the-scientist.com/atom/latest', 'The Scientist', 'News'),
        ('https://www.science.org/rss/news_current.xml', 'Science', 'News'),
        ('https://www.nature.com/nature/rss/newsandcomment', 'Nature (News & Comment)', 'News'),

        # Bio industry news
        ('https://www.statnews.com/feed/', 'STAT News', 'News'),
        ('https://arstechnica.com/science/feed/', 'Ars Technica', 'News'),
        ('https://www.wired.com/feed/category/science/latest/rss', 'Wired', 'News'),
        ('https://www.fiercebiotech.com/rss/xml', 'Fierce Biotech', 'News'),
        ('https://endpts.com/feed/', 'Endpoints News', 'News'),
        ('https://www.biopharmadive.com/feeds/news/', 'BioPharmaDive', 'News'),
        ('https://www.clinicaltrialsarena.com/feed/', 'Clinical Trials Arena', 'News'),

        # Neuroscience
        ('https://www.thetransmitter.org/feed/', 'The Transmitter', 'Neuroscience'),


        # Research papers
        ('https://www.science.org/action/showFeed?type=etoc&feed=rss&jc=science', 'Science (Paper)', 'Paper'),
        ('https://www.cell.com/cell/current.rss', 'Cell', 'Paper'),
        ('https://www.nature.com/neuro/current_issue/rss', 'Nature Neuroscience', 'Paper'),
        ('https://www.nature.com/nm/current_issue/rss', 'Nature Medicine', 'Paper'),
        ('https://www.nature.com/nrd/current_issue/rss', 'Nature Drug Discovery', 'Paper'),
        ('https://www.nature.com/nbt/current_issue/rss', 'Nature Biotechnology', 'Paper'),
        ('https://www.nature.com/nature/research-articles.rss', 'Nature (Paper)', 'Paper'),
        ('https://www.nejm.org/action/showFeed?jc=nejm&type=etoc&feed=rss', 'NEJM', 'Paper')
    ]

    # 3. í¬ë¡¤ë§ ë° í›„ë³´ ì„ ì •
    text_candidates = []
    youtube_candidates = []  # ìœ íŠœë¸Œ ì˜ìƒ í›„ë³´ (ì‹¤íŒ¨ ì¬ì‹œë„ + ì‹ ê·œ)

    # 3-1. ì‹¤íŒ¨ í•­ëª© ì¬ì‹œë„ (ìœ íŠœë¸Œ/í…ìŠ¤íŠ¸ ë¶„ë¥˜)
    if failed_queue:
        log(f"ì§€ë‚œ ì‹¤í–‰ ì‹¤íŒ¨ í•­ëª© {len(failed_queue)}ê°œ ì¬ì‹œë„ ëŒ€ê¸°", "INFO")
        for item in failed_queue:
            if item['url'] not in seen_urls:
                # URLì— 'youtube'ë‚˜ 'youtu.be'ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ìœ íŠœë¸Œ í›„ë³´ë¡œ ë³´ëƒ„
                if 'youtube' in item['url'].lower() or 'youtu.be' in item['url'].lower():
                    youtube_candidates.append(item)
                else:
                    text_candidates.append(item)
    
    # ìœ íŠœë¸Œ ì±„ë„
    yt_channels = [
        ('UCWgXoKQ4rl7SY9UHuAwxvzQ', 'B_ZCF YouTube', 'Video'),
        ('UCXql5C57vS4ogUt6CPEWWHA', 'ê¹€ì§€ìœ¤ì˜ ì§€ì‹Play YouTube', 'Video')
    ]
    for ch_id, src, cat in yt_channels:
        youtube_candidates.extend(scrape_youtube_videos(ch_id, src, cat))    
    
    # 3-2. ì‹ ê·œ í¬ë¡¤ë§
    text_candidates = []
    
    for url, source, cat in sources:
        items = scrape_feed(url, source, cat)
        for item in items:
            if item['url'] not in seen_urls:
                text_candidates.append(item)

    # ì¤‘ë³µ ì œê±°
    unique_text_candidates = list({v['url']: v for v in text_candidates}.values())

    # 1ë‹¨ê³„: seen_urls(ê³¼ê±° ê¸°ë¡)ì— ìˆëŠ” ê²ƒ ë¨¼ì € ì œì™¸
    filtered_candidates = [v for v in youtube_candidates if v['url'] not in seen_urls]
    
    # 2ë‹¨ê³„: ë‚¨ì€ ê²ƒë“¤ ì¤‘ì—ì„œ URL ê¸°ì¤€ìœ¼ë¡œ ìì²´ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ë¡œì§ í™œìš©)
    unique_youtube_candidates = list({v['url']: v for v in filtered_candidates}.values()) 
    
    # 4. AI ì²˜ë¦¬ (19ê°œ ë¸”ë¡ ë¶„í•  ì „ëµ)
    new_articles = []
    new_failed_queue = []
    
    TARGET_BLOCKS = 10  # ëª©í‘œ ìš”ì²­ íšŸìˆ˜
    
    # í…ìŠ¤íŠ¸ ê¸°ì‚¬ê°€ í•˜ë‚˜ë¼ë„ ìˆì„ ë•Œë§Œ ì²˜ë¦¬
    if unique_text_candidates:       
        article_chunks = split_into_n_chunks(unique_text_candidates, TARGET_BLOCKS)
        
        log(f"--- í…ìŠ¤íŠ¸ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘ (ì´ {len(unique_text_candidates)}ê°œ -> {len(article_chunks)}ê°œ ë¸”ë¡ìœ¼ë¡œ ë¶„í• ) ---", "INFO")
        
        for idx, batch in enumerate(article_chunks):
            log(f"ğŸ“¡ ë¸”ë¡ {idx+1}/{len(article_chunks)} ì²˜ë¦¬ ì¤‘ (ê¸°ì‚¬ {len(batch)}ê°œ í¬í•¨)...")
            
            # ë°°ì¹˜ ìš”ì•½ ì‹¤í–‰
            processed = get_gemini_batch_summary(batch)
            
            for art in processed:
                if "[ìš”ì•½ ì‹¤íŒ¨]" in art.get('summary_kr', ''):
                    new_failed_queue.append(art)
                else:
                    new_articles.append(art)
            
            # ë§ˆì§€ë§‰ ë¸”ë¡ì´ ì•„ë‹ˆë©´ 61ì´ˆ ëŒ€ê¸° (RPD ë³´ì¡´ + TPM ì¡°ì ˆ)
            if idx < len(article_chunks) - 1:
                log("â³ ë‹¤ìŒ ë¸”ë¡ ì²˜ë¦¬ë¥¼ ìœ„í•´ 300ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...", "INFO")
                time.sleep(300)
    else:
        log("ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.", "INFO")
        
    # [B] ìœ íŠœë¸Œ ì˜ìƒ ê°œë³„ ì²˜ë¦¬ (RPD ì—¬ìœ ê°€ ì—†ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ëŒ€ê¸°ì‹œê°„ ì—†ì´ ê°€ê±°ë‚˜ ìƒëµ ê³ ë ¤)
    # í•˜ì§€ë§Œ ì‚¬ìš©ì ìš”ì²­ëŒ€ë¡œ ê¸°ì¡´ ë¡œì§ ìœ ì§€
    if unique_youtube_candidates:
        log(f"--- ìœ íŠœë¸Œ ì˜ìƒ ì²˜ë¦¬ ì‹œì‘ ({len(unique_youtube_candidates)}ê±´) ---", "INFO")
        for art in unique_youtube_candidates:
            # ìœ íŠœë¸Œ ì²˜ë¦¬ ì „ ì•ˆì „ ëŒ€ê¸° (ì„ íƒì‚¬í•­)
            time.sleep(5)
            
            title_kr, summary_kr = get_gemini_summary_youtube(art)
            
            if "[ìš”ì•½ ì‹¤íŒ¨]" in summary_kr:
                new_failed_queue.append(art)
            else:
                art['title'] = title_kr
                art['summary_kr'] = summary_kr
                if 'description_en' in art: del art['description_en']
                new_articles.append(art)

    # 5. ê²°ê³¼ ì €ì¥
    log(f"ìµœì¢… ê²°ê³¼: ì„±ê³µ {len(new_articles)}ê±´, ì‹¤íŒ¨/ë³´ë¥˜ {len(new_failed_queue)}ê±´", "INFO")

    final_list = old_articles + new_articles
    final_list.sort(key=lambda x: x.get('date', ''), reverse=True)

    output_data = {
        'last_updated': datetime.now(PALO_ALTO_TZ).strftime('%Y-%m-%d %H:%M:%S'),
        'failed_queue': new_failed_queue,
        'articles': final_list
    }

    try:
        with open('articles.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        log("ë°ì´í„° ì €ì¥ ì™„ë£Œ (articles.json)", "INFO")
    except Exception as e:
        log(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}", "ERROR")
        

    # 6. logs.json ë³„ë„ ì €ì¥ (ë‚ ì§œë³„ ëˆ„ì )
    log_file_path = 'logs.json'
    all_logs = {}
    
    # ì˜¤ëŠ˜ ë‚ ì§œ í‚¤ ìƒì„± (Palo Alto ì‹œê°„ ê¸°ì¤€)
    current_date_key = start_time.strftime('%Y-%m-%d')

    if os.path.exists(log_file_path):
        try:
            with open(log_file_path, 'r', encoding='utf-8') as f:
                all_logs = json.load(f)
                # ë§Œì•½ íŒŒì¼ ë‚´ìš©ì´ dictê°€ ì•„ë‹ˆë©´ ì´ˆê¸°í™”
                if not isinstance(all_logs, dict):
                    all_logs = {}
        except Exception as e:
            print(f"ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œë¨): {e}")
            all_logs = {}
            
    # ì˜¤ëŠ˜ ë‚ ì§œ í‚¤ì— í˜„ì¬ê¹Œì§€ ìŒ“ì¸ ë¡œê·¸(execution_logs) ì €ì¥
    all_logs[current_date_key] = execution_logs

    try:
        with open(log_file_path, 'w', encoding='utf-8') as f:
            json.dump(all_logs, f, ensure_ascii=False, indent=2)
        print(f"ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {log_file_path} (Key: {current_date_key})")
    except Exception as e:
        print(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    print("=== ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ===")

if __name__ == '__main__':
    main()
