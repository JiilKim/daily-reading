#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤/ë…¼ë¬¸/ì˜ìƒì„ í¬ë¡¤ë§í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ë§¤ì¼ ìë™ ì‹¤í–‰ë˜ì–´ articles.jsonì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import feedparser
import time

def scrape_nature_news():
    """Nature ìµœì‹  ë‰´ìŠ¤ í¬ë¡¤ë§"""
    articles = []
    print("ğŸ” Nature í¬ë¡¤ë§ ì¤‘...")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # Nature ë‰´ìŠ¤ í˜ì´ì§€
        url = 'https://www.nature.com/nature/articles?type=news'
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ê¸°ì‚¬ ì°¾ê¸°
        article_items = soup.find_all('article', class_='u-full-height')[:5]
        
        for item in article_items:
            try:
                title_elem = item.find('h3')
                link_elem = item.find('a', {'data-track-action': 'view article'})
                
                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    
                    if link and not link.startswith('http'):
                        link = 'https://www.nature.com' + link
                    
                    # ê°„ë‹¨í•œ ì„¤ëª… ì¶”ì¶œ
                    desc_elem = item.find('div', class_='c-card__summary')
                    description = desc_elem.get_text(strip=True) if desc_elem else ''
                    
                    articles.append({
                        'id': abs(hash(link)),
                        'title': title,
                        'url': link,
                        'source': 'Nature',
                        'category': 'Neuroscience',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary_kr': description[:200] + '...' if len(description) > 200 else description
                    })
                    print(f"  âœ“ {title[:50]}...")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        time.sleep(1)  # ì„œë²„ ë¶€ë‹´ ì¤„ì´ê¸°
        
    except Exception as e:
        print(f"âŒ Nature í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return articles


def scrape_science_news():
    """Science.org ìµœì‹  ë‰´ìŠ¤ í¬ë¡¤ë§"""
    articles = []
    print("ğŸ” Science í¬ë¡¤ë§ ì¤‘...")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        url = 'https://www.science.org/news/all-news'
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ê¸°ì‚¬ ì°¾ê¸°
        article_items = soup.find_all('div', class_='card-header')[:5]
        
        for item in article_items:
            try:
                title_elem = item.find('h3')
                link_elem = item.find('a')
                
                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    
                    if link and not link.startswith('http'):
                        link = 'https://www.science.org' + link
                    
                    articles.append({
                        'id': abs(hash(link)),
                        'title': title,
                        'url': link,
                        'source': 'Science',
                        'category': 'Neuroscience',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary_kr': f"{title}ì— ëŒ€í•œ ìµœì‹  ê³¼í•™ ë‰´ìŠ¤ì…ë‹ˆë‹¤."
                    })
                    print(f"  âœ“ {title[:50]}...")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        time.sleep(1)
        
    except Exception as e:
        print(f"âŒ Science í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return articles


def scrape_thetransmitter():
    """The Transmitter (ì‹ ê²½ê³¼í•™ ì „ë¬¸ ë‰´ìŠ¤) í¬ë¡¤ë§"""
    articles = []
    print("ğŸ” The Transmitter í¬ë¡¤ë§ ì¤‘...")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        url = 'https://www.thetransmitter.org/'
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ê¸°ì‚¬ ì°¾ê¸°
        article_items = soup.find_all('article')[:5]
        
        for item in article_items:
            try:
                title_elem = item.find('h2') or item.find('h3')
                link_elem = item.find('a')
                
                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    
                    if link and not link.startswith('http'):
                        link = 'https://www.thetransmitter.org' + link
                    
                    articles.append({
                        'id': abs(hash(link)),
                        'title': title,
                        'url': link,
                        'source': 'The Transmitter',
                        'category': 'Neuroscience',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary_kr': f"ì‹ ê²½ê³¼í•™ ê´€ë ¨ ìµœì‹  ì—°êµ¬ ë° ë‰´ìŠ¤: {title}"
                    })
                    print(f"  âœ“ {title[:50]}...")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        time.sleep(1)
        
    except Exception as e:
        print(f"âŒ The Transmitter í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return articles


def scrape_youtube_channel(channel_id, channel_name="YouTube"):
    """ìœ íŠœë¸Œ ì±„ë„ ìµœì‹  ì˜ìƒ í¬ë¡¤ë§ (RSS í”¼ë“œ ì‚¬ìš©)"""
    videos = []
    print(f"ğŸ” YouTube ({channel_name}) í¬ë¡¤ë§ ì¤‘...")
    
    try:
        rss_url = f'https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}'
        feed = feedparser.parse(rss_url)
        
        for entry in feed.entries[:3]:  # ìµœì‹  3ê°œ
            try:
                title = entry.title
                link = entry.link
                published = entry.published if hasattr(entry, 'published') else datetime.now().strftime('%Y-%m-%d')
                
                # ë‚ ì§œ í¬ë§· ë³€í™˜
                try:
                    pub_date = datetime.strptime(published, '%Y-%m-%dT%H:%M:%S%z')
                    date_str = pub_date.strftime('%Y-%m-%d')
                except:
                    date_str = datetime.now().strftime('%Y-%m-%d')
                
                videos.append({
                    'id': abs(hash(link)),
                    'title': title,
                    'url': link,
                    'source': 'YouTube',
                    'category': 'Video',
                    'date': date_str,
                    'summary_kr': f"{channel_name} ì±„ë„ì˜ ìµœì‹  ì˜ìƒì…ë‹ˆë‹¤."
                })
                print(f"  âœ“ {title[:50]}...")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        time.sleep(1)
        
    except Exception as e:
        print(f"âŒ YouTube í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return videos


def scrape_nature_neuroscience_papers():
    """Nature Neuroscience ìµœì‹  ë…¼ë¬¸"""
    articles = []
    print("ğŸ” Nature Neuroscience ë…¼ë¬¸ í¬ë¡¤ë§ ì¤‘...")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        url = 'https://www.nature.com/neuro/research-articles'
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        article_items = soup.find_all('article')[:3]
        
        for item in article_items:
            try:
                title_elem = item.find('h3')
                link_elem = item.find('a')
                
                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    
                    if link and not link.startswith('http'):
                        link = 'https://www.nature.com' + link
                    
                    articles.append({
                        'id': abs(hash(link)),
                        'title': title,
                        'url': link,
                        'source': 'Nature Neuroscience',
                        'category': 'Neuroscience',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary_kr': f"Nature Neuroscienceì— ê²Œì¬ëœ ìµœì‹  ë…¼ë¬¸: {title[:100]}"
                    })
                    print(f"  âœ“ {title[:50]}...")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        time.sleep(1)
        
    except Exception as e:
        print(f"âŒ Nature Neuroscience í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return articles


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ“° ì¼ì¼ ì½ì„ê±°ë¦¬ ìë™ ìˆ˜ì§‘ ì‹œì‘")
    print("="*60 + "\n")
    
    all_articles = []
    
    # ê° ì†ŒìŠ¤ì—ì„œ í¬ë¡¤ë§
    all_articles.extend(scrape_nature_news())
    all_articles.extend(scrape_science_news())
    all_articles.extend(scrape_thetransmitter())
    all_articles.extend(scrape_nature_neuroscience_papers())
    
    # ìœ íŠœë¸Œ ì±„ë„ë“¤
    youtube_channels = [
        ('UCsJ6RuBiTVWRX156FVbeaGg', 'ì‹ ê²½ê³¼í•™ ì±„ë„ 1'),
        # ì—¬ê¸°ì— ë” ì¶”ê°€ ê°€ëŠ¥
    ]
    
    for channel_id, channel_name in youtube_channels:
        all_articles.extend(scrape_youtube_channel(channel_id, channel_name))
    
    # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
    seen_urls = set()
    unique_articles = []
    for article in all_articles:
        if article['url'] not in seen_urls:
            seen_urls.add(article['url'])
            unique_articles.append(article)
    
    # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    unique_articles.sort(key=lambda x: x['date'], reverse=True)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'articles': unique_articles
    }
    
    with open('articles.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print("\n" + "="*60)
    print(f"âœ… ì™„ë£Œ! ì´ {len(unique_articles)}ê°œ í•­ëª© ìˆ˜ì§‘")
    print(f"ğŸ“ articles.json íŒŒì¼ ì—…ë°ì´íŠ¸ë¨")
    print(f"ğŸ• ì—…ë°ì´íŠ¸ ì‹œê°„: {output['last_updated']}")
    print("="*60 + "\n")
    
    # ì†ŒìŠ¤ë³„ í†µê³„
    sources = {}
    for article in unique_articles:
        source = article['source']
        sources[source] = sources.get(source, 0) + 1
    
    print("ğŸ“Š ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ í˜„í™©:")
    for source, count in sources.items():
        print(f"  â€¢ {source}: {count}ê°œ")


if __name__ == '__main__':
    main()
