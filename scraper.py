#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¼ì¼ ê³¼í•™ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (Gemini AI ë²ˆì—­ ë° ìš”ì•½ ê¸°ëŠ¥ í¬í•¨)
- RSS í”¼ë“œ ë° ìœ íŠœë¸Œ ì±„ë„ í¬ë¡¤ë§
- Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸  ë²ˆì—­ ë° ìš”ì•½
- URL ì»¨í…ìŠ¤íŠ¸ë¥¼ í†µí•œ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ ì§€ì›
- ìµœê·¼ 7ì¼ê°„ì˜ ì•„ì¹´ì´ë¸Œ ìœ ì§€
- GitHub Actions í˜¸í™˜
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import feedparser
import time
import os
from google import genai
from google.genai import types
from urllib.parse import urljoin
import sys

# ============================================================================
# ì„¤ì •
# ============================================================================

MAX_NEW_ARTICLES_PER_RUN = 8000
ARCHIVE_DAYS = 7
API_DELAY_SECONDS = 1

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'application/xml,application/rss+xml,text/xml;q=0.9,*/*;q=0.5',
    'Accept-Language': 'en-US,en;q=0.9,ko;q=0.8',
    'Cache-Control': 'no-cache',
}

# ============================================================================
# AI ë²ˆì—­ ë° ìš”ì•½
# ============================================================================

def get_gemini_summary(article_data):
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ì½˜í…ì¸ ë¥¼ ë²ˆì—­í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.
    ìœ íŠœë¸Œ ì˜ìƒì˜ ê²½ìš° URLì„ í†µí•´ ì§ì ‘ ì˜ìƒ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        article_data (dict): title_en, description_en, url, sourceë¥¼ í¬í•¨í•œ ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°
        
    Returns:
        tuple: (translated_title_kr, summary_kr)
    """
    title_en = article_data['title_en']
    description_en = article_data['description_en']
    url = article_data['url']
    source = article_data.get('source', '')

    try:
        api_key = os.environ.get('GEMINI_API_KEY')
        
        if not api_key:
            print("  [AI] âŒ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í‚¤ ì—†ìŒ. (ì›ë³¸: {description_en[:100]}...)"

        client = genai.Client(api_key=api_key)

        # ìœ íŠœë¸Œ ì˜ìƒ: URLì„ í†µí•´ ì§ì ‘ ì˜ìƒ ì½˜í…ì¸  ë¶„ì„
        if 'YouTube' in source:
            print(f"  [AI] ğŸ¥ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ ì¤‘: '{title_en[:40]}...'")
            
            prompt = f"""
ë‹¹ì‹ ì€ ì˜ìƒ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ ìœ íŠœë¸Œ ì˜ìƒì„ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.
ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.

[ì…ë ¥]
- title_en: "{title_en}"

[JSON ì¶œë ¥ í˜•ì‹]
{{
  "title_kr": "ì—¬ê¸°ì— ì œëª©ì˜ ì „ë¬¸ì ì¸ í•œêµ­ì–´ ë²ˆì—­ì„ ì‘ì„±í•©ë‹ˆë‹¤",
  "summary_kr": "í•µì‹¬ ìš”ì ì„ ì¶”ì¶œí•˜ì—¬, ì˜ìƒ ì½˜í…ì¸ ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ìµœì†Œ 10ë¬¸ì¥ ë¶„ëŸ‰ì˜ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤"
}}

[ê·œì¹™]
1. "title_kr": "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
2. "summary_kr": ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì²´ë¡œ ìƒì„¸í•œ ìµœì†Œ 10ë¬¸ì¥ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
3. ëŒ€í™”ì²´ê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ê¸€ì“°ê¸° ë¬¸ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

            response = client.models.generate_content(
                model='gemini-2.5-flash', # ëª¨ë¸ ë²„ì „
                contents=[
                    prompt,
                    types.Part.from_uri(
                        file_uri=url,
                        mime_type="video/youtube"
                    )
                ],
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            
        # í…ìŠ¤íŠ¸ ê¸°ì‚¬: ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ë²ˆì—­ ë° ìš”ì•½
        else:
            print(f"  [AI] ğŸ“ ê¸°ì‚¬ ë²ˆì—­ ì¤‘: '{title_en[:40]}...'")
            
            prompt = f"""
ë‹¹ì‹ ì€ ê³¼í•™ì— ëŠ¥í†µí•œ ì „ë¬¸ ê¸°ì í˜¹ì€ ì»¤ë®¤ë‹ˆì¼€ì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ì˜ì–´ ê¸°ì‚¬ ì œëª©ê³¼ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ, í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
 
[ì…ë ¥]
- title_en: "{title_en}"
- description_en: "{description_en}"

[JSON ì¶œë ¥ í˜•ì‹]
{{
  "title_kr": "ì—¬ê¸°ì— í•œêµ­ì–´ ë²ˆì—­ ì œëª©ì„ ì‘ì„±",
  "summary_kr": "ì—¬ê¸°ì— ìµœì†Œ 5-6 ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ìƒì„¸í•œ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±"
}}

[ê·œì¹™]
1. "title_kr" í‚¤ì—ëŠ” "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
2. "summary_kr" í‚¤ì—ëŠ” "description_en"ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
3. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì²´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
"""
            
            response = client.models.generate_content(
                model='gemini-2.5-flash', # ëª¨ë¸ ë²„ì „
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )

        # JSON ì‘ë‹µ íŒŒì‹±
        data = json.loads(response.text)
        title_kr = data.get('title_kr', title_en)
        summary_kr = data.get('summary_kr', f"[ìš”ì•½ ì‹¤íŒ¨] API ì˜¤ë¥˜. (ì›ë³¸: {description_en[:100]}...)")

        print(f"  [AI] âœ“ ë²ˆì—­ ì™„ë£Œ: {title_kr[:40]}...")
        return title_kr, summary_kr

    except json.JSONDecodeError as e:
        print(f"  [AI] âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] ì˜ëª»ëœ API ì‘ë‹µ. (ì›ë³¸: {description_en[:100]}...)"
    
    except Exception as e:
        print(f"  [AI] âŒ API ì˜¤ë¥˜: {e}")
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í˜¸ì¶œ ì‹¤íŒ¨. (ì›ë³¸: {description_en[:100]}...)"


# ============================================================================
# RSS í”¼ë“œ ìŠ¤í¬ë˜í¼
# ============================================================================

def scrape_rss_feed(feed_url, source_name, category_name):
    """
    ê°•ë ¥í•œ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ìŠ¤í¬ë©í•©ë‹ˆë‹¤.
    
    Args:
        feed_url (str): RSS í”¼ë“œ URL
        source_name (str): ì‹ë³„ì„ ìœ„í•œ ì†ŒìŠ¤ ì´ë¦„
        category_name (str): ê¸°ì‚¬ ì¹´í…Œê³ ë¦¬ (News/Paper/Video)
        
    Returns:
        list: ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    articles = []
    print(f"ğŸ” [{source_name}] RSS í¬ë¡¤ë§ ì¤‘... (URL: {feed_url})")

    try:
        response = requests.get(feed_url, headers=HEADERS, timeout=20)
        response.raise_for_status()

        content_type = response.headers.get('Content-Type', '').lower()
        if not any(ct in content_type for ct in ['xml', 'rss', 'atom']):
            print(f"  âŒ ì˜ëª»ëœ ì½˜í…ì¸  ìœ í˜•: {content_type}")
            print(f"     ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response.text[:200]}...")
            return []

        feed = feedparser.parse(response.content)

        if feed.bozo:
            print(f"  âš ï¸ í”¼ë“œ íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")

        print(f"  [i] {len(feed.entries)}ê°œ ì•„ì´í…œ ë°œê²¬")

        for entry in feed.entries:
            try:
                if not entry.get('title') or not entry.get('link'):
                    print("    âš ï¸ ì œëª© ë˜ëŠ” ë§í¬ ëˆ„ë½. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                title_en = entry.title
                link = entry.link
                description_en = entry.get('summary') or entry.get('description') or title_en
                description_text = BeautifulSoup(description_en, 'html.parser').get_text(strip=True)

                # ë°œí–‰ì¼ íŒŒì‹±
                date_str = datetime.now().strftime('%Y-%m-%d')
                if entry.get('published_parsed'):
                    try:
                        dt_obj = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                        date_str = dt_obj.strftime('%Y-%m-%d')
                    except (TypeError, ValueError):
                        pass

                # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                image_url = None
                if entry.get('media_thumbnail'):
                    image_url = entry.media_thumbnail[0].get('url')
                elif entry.get('links'):
                    for e_link in entry.links:
                        if e_link.get('rel') == 'enclosure' and e_link.get('type', '').startswith('image/'):
                            image_url = e_link.get('href')
                            break
                
                if not image_url and description_en:
                    desc_soup = BeautifulSoup(description_en, 'html.parser')
                    img_tag = desc_soup.find('img')
                    if img_tag and img_tag.get('src'):
                        image_url = urljoin(link, img_tag.get('src'))

                title_en = BeautifulSoup(title_en, 'html.parser').get_text(strip=True)

                articles.append({
                    'title_en': title_en,
                    'description_en': description_text,
                    'url': link,
                    'source': source_name,
                    'category': category_name,
                    'date': date_str,
                    'image_url': image_url
                })

            except Exception as item_err:
                print(f"  âœ— ì•„ì´í…œ íŒŒì‹± ì‹¤íŒ¨: {item_err}")

    except requests.exceptions.RequestException as req_err:
        print(f"âŒ [{source_name}] ìš”ì²­ ì‹¤íŒ¨: {req_err}")
    except Exception as e:
        print(f"âŒ [{source_name}] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

    return articles


# ============================================================================
# ìœ íŠœë¸Œ ì±„ë„ ìŠ¤í¬ë˜í¼
# ============================================================================

def scrape_youtube_videos(channel_id, source_name, category_name):
    """
    ìœ íŠœë¸Œ ì±„ë„ RSS í”¼ë“œì—ì„œ ìµœì‹  ë™ì˜ìƒì„ ìŠ¤í¬ë©í•©ë‹ˆë‹¤.
    ì˜ìƒ ì½˜í…ì¸ ëŠ” AIê°€ URL ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        channel_id (str): ìœ íŠœë¸Œ ì±„ë„ ID
        source_name (str): ì‹ë³„ì„ ìœ„í•œ ì†ŒìŠ¤ ì´ë¦„
        category_name (str): ê¸°ì‚¬ ì¹´í…Œê³ ë¦¬
        
    Returns:
        list: ì˜ìƒ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    articles = []
    print(f"ğŸ” [{source_name}] ìœ íŠœë¸Œ í¬ë¡¤ë§ ì¤‘... (ì±„ë„: {channel_id})")
    feed_url = f'https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}'

    try:
        response = requests.get(feed_url, headers=HEADERS, timeout=20)
        response.raise_for_status()

        content_type = response.headers.get('Content-Type', '').lower()
        if 'xml' not in content_type:
            print(f"  âŒ ì˜ëª»ëœ ì½˜í…ì¸  ìœ í˜•: {content_type}")
            return []

        feed = feedparser.parse(response.content)

        if feed.bozo:
            print(f"  âš ï¸ í”¼ë“œ íŒŒì‹± ê²½ê³ : {feed.bozo_exception}")

        print(f"  [i] {len(feed.entries)}ê°œì˜ ìµœì‹  ì˜ìƒ ë°œê²¬")

        for entry in feed.entries:
            try:
                if not entry.get('title') or not entry.get('link'):
                    print("    âš ï¸ ì œëª© ë˜ëŠ” ë§í¬ ëˆ„ë½. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                title_en = entry.title
                link = entry.link
                video_id = link.split('v=')[-1]

                # ë°œí–‰ì¼ íŒŒì‹±
                date_str = datetime.now().strftime('%Y-%m-%d')
                if entry.get('published_parsed'):
                    dt_obj = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                    date_str = dt_obj.strftime('%Y-%m-%d')

                # ê³ í™”ì§ˆ ì¸ë„¤ì¼ ê°€ì ¸ì˜¤ê¸°
                image_url = None
                if entry.get('media_thumbnail') and entry.media_thumbnail:
                    image_url = entry.media_thumbnail[0]['url'].replace('default.jpg', 'hqdefault.jpg')

                # AIë¥¼ ìœ„í•œ ë³´ì¡° ì •ë³´ë¡œ RSS ì„¤ëª… ì‚¬ìš©
                description_en = entry.get('media_description', entry.get('summary', title_en))
                description_text = BeautifulSoup(description_en, 'html.parser').get_text(strip=True)
                
                print(f"    [i] ì˜ìƒ {video_id} ë¡œë“œë¨. AIê°€ URLì„ ì§ì ‘ ë¶„ì„í•©ë‹ˆë‹¤.")

                articles.append({
                    'title_en': title_en,
                    'description_en': description_text,
                    'url': link,
                    'source': source_name,
                    'category': category_name,
                    'date': date_str,
                    'image_url': image_url
                })

            except Exception as item_err:
                print(f"  âœ— ì˜ìƒ íŒŒì‹± ì‹¤íŒ¨: {item_err}")

    except requests.exceptions.RequestException as req_err:
        print(f"âŒ [{source_name}] ìš”ì²­ ì‹¤íŒ¨: {req_err}")
    except Exception as e:
        print(f"âŒ [{source_name}] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

    return articles


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """GitHub Actions ì›Œí¬í”Œë¡œìš°ë¥¼ ìœ„í•œ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("\n" + "="*60)
    print("ğŸ“° ì¼ì¼ ê³¼í•™ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ - ì‹œì‘")
    print(f"ğŸ• ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60 + "\n")

    # ========================================================================
    # 1. ëª¨ë“  ì†ŒìŠ¤ í¬ë¡¤ë§
    # ========================================================================
    
    all_articles_to_check = []
    
    # ìœ íŠœë¸Œ ì±„ë„
    all_articles_to_check.extend(
        scrape_youtube_videos('UCWgXoKQ4rl7SY9UHuAwxvzQ', 'B_ZCF YouTube', 'Video')
    )
    
    # ë‰´ìŠ¤ ì†ŒìŠ¤
    all_articles_to_check.extend(scrape_rss_feed('https://www.thetransmitter.org/feed/', 'The Transmitter', 'Neuroscience'))    
    all_articles_to_check.extend(scrape_rss_feed('https://www.nature.com/nature/rss/articles?type=news', 'Nature', 'News'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.statnews.com/feed/', 'STAT News', 'News'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.the-scientist.com/atom/latest', 'The Scientist', 'News'))
    all_articles_to_check.extend(scrape_rss_feed('https://arstechnica.com/science/feed/', 'Ars Technica', 'News'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.wired.com/feed/category/science/latest/rss', 'Wired', 'News'))
    all_articles_to_check.extend(scrape_rss_feed('https://neurosciencenews.com/feed/', 'Neuroscience News', 'News'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.fiercebiotech.com/rss/xml', 'Fierce Biotech', 'News'))
    all_articles_to_check.extend(scrape_rss_feed('https://endpts.com/feed/', 'Endpoints News', 'News'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.science.org/rss/news_current.xml', 'Science', 'News'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.nature.com/nature/rss/newsandcomment', 'Nature (News & Comment)', 'News'))
    
    # ê³¼í•™ ë…¼ë¬¸
    all_articles_to_check.extend(scrape_rss_feed('https://www.science.org/action/showFeed?type=etoc&feed=rss&jc=science', 'Science (Paper)', 'Paper'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.cell.com/cell/current.rss', 'Cell', 'Paper'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.nature.com/neuro/current_issue/rss', 'Nature Neuroscience', 'Paper'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.nature.com/nm/current_issue/rss', 'Nature Medicine', 'Paper'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.nature.com/nrd/current_issue/rss', 'Nature Drug Discovery', 'Paper'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.nature.com/nbt/current_issue/rss', 'Nature Biotechnology', 'Paper'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.nature.com/nature/research-articles.rss', 'Nature (Paper)', 'Paper'))
    all_articles_to_check.extend(scrape_rss_feed('https://www.nejm.org/action/showFeed?jc=nejm&type=etoc&feed=rss', 'NEJM', 'Paper'))

    # ========================================================================
    # 2. ê¸°ì¡´ ê¸°ì‚¬ ë¡œë“œ (***ìˆ˜ì •ëœ ë¡œì§***)
    # ========================================================================
    
    seen_urls = set()
    old_articles_to_keep = [] # ARCHIVE_DAYS ë‚´ì˜ ê¸°ì‚¬ë§Œ ë³´ê´€í•  ì„ì‹œ ë¦¬ìŠ¤íŠ¸

    try:
        with open('articles.json', 'r', encoding='utf-8') as f:
            old_data = json.load(f)
            for old_article in old_data.get('articles', []):
                if not old_article.get('url'):
                    continue
                
                # [ìˆ˜ì •] 1. ë‚ ì§œì™€ ìƒê´€ì—†ì´ ëª¨ë“  URLì„ 'seen_urls'ì— ì¶”ê°€í•˜ì—¬ API ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
                seen_urls.add(old_article['url'])
                
                # [ìˆ˜ì •] 2. 7ì¼ ì´ë‚´ì˜ ê¸°ì‚¬ì¸ì§€ ë³„ë„ë¡œ í™•ì¸í•˜ì—¬ ìµœì¢… ëª©ë¡ì— ìœ ì§€
                try:
                    article_date = datetime.strptime(old_article.get('date', '1970-01-01'), '%Y-%m-%d')
                    if (datetime.now() - article_date).days <= ARCHIVE_DAYS:
                        old_articles_to_keep.append(old_article)
                except ValueError:
                    continue # ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš° ë¬´ì‹œ
                    
        print(f"\n[i] ê¸°ì¡´ URL {len(seen_urls)}ê°œ ë¡œë“œ (API ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€ìš©)")
        print(f"    (ê·¸ ì¤‘ {len(old_articles_to_keep)}ê°œ ê¸°ì‚¬ê°€ {ARCHIVE_DAYS}ì¼ ì´ë‚´ì´ë¯€ë¡œ ë³´ê´€)")
        
    except FileNotFoundError:
        print("\n[i] 'articles.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
    except json.JSONDecodeError:
        print("\n[i] âŒ 'articles.json' íŒŒì¼ì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        old_articles_to_keep = []
        seen_urls = set()

    # ========================================================================
    # 3. ìƒˆ ê¸°ì‚¬ AI ë²ˆì—­ ì²˜ë¦¬
    # ========================================================================
    
    new_articles = []
    existing_articles_count = 0
    new_article_count = 0
    api_errors = 0

    print(f"\n[i] {len(all_articles_to_check)}ê°œ ì•„ì´í…œ í™•ì¸ ì¤‘ (ìµœëŒ€ {MAX_NEW_ARTICLES_PER_RUN}ê°œ ìƒˆ ê¸°ì‚¬)")

    for article_data in all_articles_to_check:
        
        if not article_data.get('url'):
            print(f"  âš ï¸ URL ëˆ„ë½ (ì†ŒìŠ¤: {article_data.get('source', 'N/A')}). ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # [ìˆ˜ì •] ì´ì œ seen_urlsëŠ” ëª¨ë“  ê³¼ê±° ê¸°ì‚¬ URLì„ í¬í•¨í•˜ë¯€ë¡œ 7ì¼ì´ ì§€ë‚œ ê¸°ì‚¬ë„ API í˜¸ì¶œì„ ê±´ë„ˆëœ€
        if article_data['url'] not in seen_urls:
            
            if new_article_count >= MAX_NEW_ARTICLES_PER_RUN:
                print(f"  [i] ìµœëŒ€ ê°œìˆ˜ ({MAX_NEW_ARTICLES_PER_RUN}ê°œ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í• ë‹¹ëŸ‰ ë³´í˜¸ë¥¼ ìœ„í•´ ì¤‘ì§€í•©ë‹ˆë‹¤.")
                break

            new_article_count += 1
            print(f"  [i] âœ¨ ìƒˆ ì•„ì´í…œ ë°œê²¬ ({new_article_count}/{MAX_NEW_ARTICLES_PER_RUN}): {article_data['title_en'][:50]}...")

            # AIë¡œ ë²ˆì—­ ë° ìš”ì•½
            title_kr, summary_kr = get_gemini_summary(article_data)

            # [ì‚¬ìš©ì ìš”ì²­] ë²ˆì—­/ìš”ì•½ì— ì‹¤íŒ¨í•œ ê¸°ì‚¬ëŠ” ê±´ë„ˆëœ€
            if "[Translation Failed]" in summary_kr or "[ìš”ì•½ ì‹¤íŒ¨]" in summary_kr:
                api_errors += 1
                print(f"  [i] âŒ AI ì‹¤íŒ¨. ê¸°ì‚¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {article_data['title_en'][:50]}...")
            else:
                # ìµœì¢… ê¸°ì‚¬ ê°ì²´ ì¤€ë¹„
                article_data['title'] = title_kr
                article_data['summary_kr'] = summary_kr
                article_data['summary_en'] = article_data['description_en']
                del article_data['description_en']

                new_articles.append(article_data)
                seen_urls.add(article_data['url']) # í˜¹ì‹œë‚˜ ì¤‘ë³µ ìˆ˜ì§‘ë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì—¬ê¸°ì„œë„ ì¶”ê°€

            time.sleep(API_DELAY_SECONDS)

        else:
            existing_articles_count += 1

    print(f"\n[i] ìƒˆ ê¸°ì‚¬ {new_article_count}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"    (ì„±ê³µ: {new_article_count - api_errors}, API ì˜¤ë¥˜: {api_errors})")
    print(f"    (ê¸°ì¡´ ê¸°ì‚¬ {existing_articles_count}ê°œ ê±´ë„ˆëœ€)")

    # ========================================================================
    # 4. ê¸°ì‚¬ ë³‘í•© ë° ì¤‘ë³µ ì œê±° (***ìˆ˜ì •ëœ ë¡œì§***)
    # ========================================================================
    
    # [ìˆ˜ì •] final_article_list ëŒ€ì‹  old_articles_to_keepì—ì„œ ì‹œì‘
    deduplicated_list = old_articles_to_keep
    deduplicated_list.extend(new_articles)

    # ë‚¨ì€ ì¤‘ë³µ í•­ëª© ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš° ëŒ€ë¹„)
    final_seen_urls = set()
    final_deduplicated_list = []
    
    for article in deduplicated_list:
        if article.get('url') and article['url'] not in final_seen_urls:
            # [ì‚¬ìš©ì ìš”ì²­] ë²ˆì—­/ìš”ì•½ ì‹¤íŒ¨ í•­ëª©ì´ ê¸°ì¡´ ëª©ë¡ì— ìˆë”ë¼ë„ ìµœì¢… ëª©ë¡ì—ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            if "[Translation Failed]" not in article.get('summary_kr', '') and "[ìš”ì•½ ì‹¤íŒ¨]" not in article.get('summary_kr', ''):
                final_seen_urls.add(article['url'])
                final_deduplicated_list.append(article)
            else:
                print(f"  [i] ğŸ—‘ï¸ ê¸°ì¡´ ëª©ë¡ì—ì„œ ì‹¤íŒ¨í•œ í•­ëª© ì œê±°: {article.get('title', 'N/A')[:50]}...")

    # [ìˆ˜ì •] ìµœì¢… ë¦¬ìŠ¤íŠ¸ë¥¼ í• ë‹¹
    deduplicated_list = final_deduplicated_list

    # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    deduplicated_list.sort(key=lambda x: x.get('date', '1970-01-01'), reverse=True)

    # ========================================================================
    # 5. JSON íŒŒì¼ë¡œ ì €ì¥
    # ========================================================================
    
    output = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'articles': deduplicated_list
    }

    json_file_path = 'articles.json'
    try:
        with open(json_file_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ì„±ê³µ! {len(deduplicated_list)}ê°œ ê¸°ì‚¬ ì €ì¥ (ìµœê·¼ {ARCHIVE_DAYS}ì¼ + ì‹ ê·œ)")
        print(f"ğŸ“ '{json_file_path}' ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    except Exception as write_err:
        print(f"\nâŒ JSON ì €ì¥ ì‹¤íŒ¨: {write_err}")
        sys.exit(1)

    # ========================================================================
    # 6. í†µê³„ ì¶œë ¥
    # ========================================================================
    
    print("\n" + "="*60)
    print(f"ğŸ“Š ìˆ˜ì§‘ í†µê³„ (ìµœê·¼ {ARCHIVE_DAYS}ì¼ + ì‹ ê·œ):")
    print("="*60)
    
    sources = {}
    for article in deduplicated_list:
        source = article.get('source', 'Unknown')
        sources[source] = sources.get(source, 0) + 1

    for source, count in sorted(sources.items()):
        print(f"  â€¢ {source}: {count} articles")
    
    print("\n" + "="*60)
    print(f"ğŸ• ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60 + "\n")


if __name__ == '__main__':
    main()

