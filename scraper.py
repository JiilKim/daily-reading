#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤/ë…¼ë¬¸/ì˜ìƒì„ í¬ë¡¤ë§í•˜ê³ 
Gemini APIë¥¼ ì´ìš©í•´ ë²ˆì—­/ìš”ì•½í•œ í›„
articles.jsonì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import feedparser
import time
import os
import google.generativeai as genai
import google.generativeai.types as genai_types

# --- AI ìš”ì•½ ê¸°ëŠ¥ (JSON í¬ë§·ìœ¼ë¡œ ìˆ˜ì •) ---

def get_gemini_summary(title_en, description_en):
    """
    Gemini APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì œëª©ê³¼ ì„¤ëª…ì„ í•œê¸€ë¡œ ë²ˆì—­ ë° ìš”ì•½í•©ë‹ˆë‹¤.
    ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
    """
    print(f"  [AI] '{title_en[:30]}...' ë²ˆì—­/ìš”ì•½ ìš”ì²­ ì¤‘...")
    
    try:
        # API í‚¤ëŠ” GitHub Actions Secretsì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        api_key = os.environ.get('GEMINI_API_KEY')
        
        if not api_key:
            print("  [AI] âŒ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ì›ë³¸: {description_en[:100]}...)"

        genai.configure(api_key=api_key)
        
        # JSON ì‘ë‹µì„ ìœ„í•œ ì„¤ì •
        generation_config = genai.GenerationConfig(response_mime_type="application/json")
        model = genai.GenerativeModel(
            'gemini-2.5-flash-preview-09-2025',
            generation_config=generation_config
        )
        
        prompt = f"""
        ë‹¹ì‹ ì€ ì „ë¬¸ ê³¼í•™ ë‰´ìŠ¤ í¸ì§‘ìì…ë‹ˆë‹¤.
        ì•„ë˜ì˜ ì˜ì–´ ê¸°ì‚¬ ì œëª©ê³¼ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ, í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
        ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

        [ì…ë ¥]
        - title_en: "{title_en}"
        - description_en: "{description_en}"

        [JSON ì¶œë ¥ í˜•ì‹]
        {{
          "title_kr": "ì—¬ê¸°ì— í•œêµ­ì–´ ë²ˆì—­ ì œëª©ì„ ì‘ì„±",
          "summary_kr": "ì—¬ê¸°ì— 5-6 ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ìƒì„¸í•œ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±"
        }}

        [ê·œì¹™]
        1. "title_kr" í‚¤ì—ëŠ” "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
        2. "summary_kr" í‚¤ì—ëŠ” "description_en"ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ 5-6 ë¬¸ì¥ì˜ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
        3. ì¹œì ˆí•œ ë§íˆ¬ê°€ ì•„ë‹Œ, ì „ë¬¸ì ì´ê³  ê°„ê²°í•œ ë‰´ìŠ¤ì²´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        """
        
        response = model.generate_content(prompt)
        
        # JSON íŒŒì‹±
        data = json.loads(response.text)
        
        title_kr = data.get('title_kr', title_en)
        summary_kr = data.get('summary_kr', f"[ìš”ì•½ ì‹¤íŒ¨] API ì‘ë‹µ ì˜¤ë¥˜. (ì›ë³¸: {description_en[:100]}...)")
        
        print(f"  [AI] âœ“ ìš”ì•½ ì™„ë£Œ: {title_kr[:30]}...")
        return title_kr, summary_kr
    
    except Exception as e:
        print(f"  [AI] âŒ Gemini API ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ì–´ ì›ë³¸ ë°˜í™˜
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. (ì›ë³¸: {description_en[:100]}...)"
    except json.JSONDecodeError as e:
        print(f"  [AI] âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}. ì‘ë‹µ í…ìŠ¤íŠ¸: {response.text[:100]}...")
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜. (ì›ë³¸: {description_en[:100]}...)"


# --- ì›¹ì‚¬ì´íŠ¸ë³„ ìŠ¤í¬ë˜í¼ (ê¸°ì‚¬ ìˆ˜ ì œí•œ ì œê±°) ---

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def scrape_nature_news():
    """Nature ìµœì‹  ë‰´ìŠ¤ í¬ë¡¤ë§ (ì œí•œ ì—†ìŒ)"""
    articles = []
    print("ğŸ” [Nature News] í¬ë¡¤ë§ ì¤‘...")
    
    try:
        url = 'https://www.nature.com/nature/articles?type=news'
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        # [ìˆ˜ì •] ê¸°ì‚¬ ìˆ˜ ì œí•œ ì œê±°
        article_items = soup.find_all('article', class_='u-full-height')
        
        for item in article_items:
            try:
                title_elem = item.find('h3')
                link_elem = item.find('a', {'data-track-action': 'view article'})
                
                if title_elem and link_elem:
                    title_en = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if link and not link.startswith('http'):
                        link = 'https://www.nature.com' + link
                    
                    desc_elem = item.find('div', class_='c-card__summary')
                    description_en = desc_elem.get_text(strip=True) if desc_elem else ''
                    
                    # [ìˆ˜ì •] title_kr, summary_kr ë°˜í™˜
                    title_kr, summary_kr = get_gemini_summary(title_en, description_en)
                    
                    articles.append({
                        'title': title_kr,       # í•œêµ­ì–´ ì œëª©
                        'title_en': title_en,    # (ì°¸ê³ ìš©) ì˜ì–´ ì›ë³¸ ì œëª©
                        'url': link,
                        'source': 'Nature',
                        'category': 'Science News',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary_kr': summary_kr
                    })
                    print(f"  âœ“ {title_en[:50]}... -> {title_kr[:30]}...")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
            time.sleep(1) # API í˜¸ì¶œ ë”œë ˆì´
            
    except Exception as e:
        print(f"âŒ [Nature News] í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return articles

def scrape_science_news():
    """Science.org ìµœì‹  ë‰´ìŠ¤ í¬ë¡¤ë§ (ì œí•œ ì—†ìŒ)"""
    articles = []
    print("ğŸ” [Science News] í¬ë¡¤ë§ ì¤‘...")
    
    try:
        url = 'https://www.science.org/news'
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # [ìˆ˜ì •] ê¸°ì‚¬ ìˆ˜ ì œí•œ ì œê±°
        article_items = soup.select('div.card-data')
        
        for item in article_items:
            try:
                title_elem = item.find('h2', class_='card-title')
                link_elem = item.find('a')
                
                if title_elem and link_elem:
                    title_en = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    if link and not link.startswith('http'):
                        link = 'https://www.science.org' + link
                    
                    desc_elem = item.find('p', class_='card-summary')
                    description_en = desc_elem.get_text(strip=True) if desc_elem else title_en
                    
                    title_kr, summary_kr = get_gemini_summary(title_en, description_en)

                    articles.append({
                        'title': title_kr,
                        'title_en': title_en,
                        'url': link,
                        'source': 'Science',
                        'category': 'Science News',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary_kr': summary_kr
                    })
                    print(f"  âœ“ {title_en[:50]}... -> {title_kr[:30]}...")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
            time.sleep(1)
            
    except Exception as e:
        print(f"âŒ [Science News] í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return articles

def scrape_cell_news():
    """Cell.com ìµœì‹  ë‰´ìŠ¤ (RSS í”¼ë“œ ì‚¬ìš©) (ì œí•œ ì—†ìŒ)"""
    articles = []
    print("ğŸ” [Cell News] (RSS) í¬ë¡¤ë§ ì¤‘...")
    
    try:
        rss_url = 'https://www.cell.com/rss/cell-news.xml'
        feed = feedparser.parse(rss_url)
        
        # [ìˆ˜ì •] ê¸°ì‚¬ ìˆ˜ ì œí•œ ì œê±°
        for entry in feed.entries:
            try:
                title_en = entry.title
                link = entry.link
                description_en = entry.summary
                
                description_text = BeautifulSoup(description_en, 'html.parser').get_text(strip=True)
                
                title_kr, summary_kr = get_gemini_summary(title_en, description_text)
                
                pub_date = entry.published_parsed if hasattr(entry, 'published_parsed') else datetime.now().timetuple()
                date_str = datetime.fromtimestamp(time.mktime(pub_date)).strftime('%Y-%m-%d')

                articles.append({
                    'title': title_kr,
                    'title_en': title_en,
                    'url': link,
                    'source': 'Cell',
                    'category': 'Science News',
                    'date': date_str,
                    'summary_kr': summary_kr
                })
                print(f"  âœ“ {title_en[:50]}... -> {title_kr[:30]}...")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
            time.sleep(1)
            
    except Exception as e:
        print(f"âŒ [Cell News] í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return articles

def scrape_thetransmitter():
    """[ìˆ˜ì •] The Transmitter (ì‹ ê²½ê³¼í•™ ì „ë¬¸ ë‰´ìŠ¤) í¬ë¡¤ë§ (ì œí•œ ì—†ìŒ)"""
    articles = []
    print("ğŸ” [The Transmitter] í¬ë¡¤ë§ ì¤‘...")
    
    try:
        url = 'https://www.thetransmitter.org/news/'
        response = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # [ìˆ˜ì •] ì„ íƒì ë³€ê²½ ë° ê¸°ì‚¬ ìˆ˜ ì œí•œ ì œê±°
        article_items = soup.select('div.hp-post-card')
        
        if not article_items:
             print("  [i] 'hp-post-card' ì„ íƒì ì—†ìŒ. 'article'ë¡œ ì¬ì‹œë„...")
             article_items = soup.find_all('article')
             
        print(f"  [i] {len(article_items)}ê°œ ê¸°ì‚¬ ì¹´ë“œ ì°¾ìŒ")

        for item in article_items:
            try:
                # [ìˆ˜ì •] ìƒˆë¡œìš´ ì„ íƒì
                title_elem_a = item.select_one('h3.hp-post-card__title a')
                
                # ëŒ€ì²´ ì„ íƒì (ê¸°ì¡´ ë°©ì‹)
                if not title_elem_a:
                     title_elem_a = item.find('h3').find('a') if item.find('h3') else None

                if title_elem_a:
                    title_en = title_elem_a.get_text(strip=True)
                    link = title_elem_a.get('href', '')
                    
                    if link and not link.startswith('http'):
                        link = 'https://www.thetransmitter.org' + link
                    
                    # [ìˆ˜ì •] ìƒˆë¡œìš´ ì„ íƒì
                    desc_elem = item.select_one('p.hp-post-card__excerpt')
                    # ëŒ€ì²´ ì„ íƒì (ê¸°ì¡´ ë°©ì‹)
                    if not desc_elem:
                        desc_elem = item.find('p')
                        
                    description_en = desc_elem.get_text(strip=True) if desc_elem else title_en
                    
                    title_kr, summary_kr = get_gemini_summary(title_en, description_en)
                    
                    articles.append({
                        'title': title_kr,
                        'title_en': title_en,
                        'url': link,
                        'source': 'The Transmitter',
                        'category': 'Neuroscience',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary_kr': summary_kr
                    })
                    print(f"  âœ“ {title_en[:50]}... -> {title_kr[:30]}...")
                else:
                    print("  âœ— í•­ëª©ì—ì„œ ì œëª©/ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
            time.sleep(1)
            
    except Exception as e:
        print(f"âŒ [The Transmitter] í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return articles

def scrape_nature_journal(journal_name, journal_code, category):
    """Nature ìë§¤ì§€ í¬ë¡¤ë§ (ì œí•œ ì—†ìŒ)"""
    articles = []
    print(f"ğŸ” [Nature {journal_name}] í¬ë¡¤ë§ ì¤‘...")
    
    try:
        url = f'https://www.nature.com/{journal_code}/news-and-comment'
        response = requests.get(url, headers=HEADERS, timeout=15)
        
        if response.status_code == 404:
            print(f"  [i] 'news-and-comment' ì—†ìŒ, 'research'ë¡œ ì¬ì‹œë„...")
            url = f'https://www.nature.com/{journal_code}/research-articles'
            response = requests.get(url, headers=HEADERS, timeout=15)
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # [ìˆ˜ì •] ê¸°ì‚¬ ìˆ˜ ì œí•œ ì œê±°
        article_items = soup.find_all('article')
        
        if not article_items:
             article_items = soup.find_all('li', class_='app-article-list-row__item')

        for item in article_items:
            try:
                title_elem = item.find('h3')
                link_elem = item.find('a')
                
                if title_elem and link_elem:
                    title_en = title_elem.get_text(strip=True)
                    link = link_elem.get('href', '')
                    
                    if link and not link.startswith('http'):
                        link = 'https://www.nature.com' + link
                    
                    desc_elem = item.find(['p', 'div'], class_=['c-card__summary', 'app-article-list-row__summary'])
                    description_en = desc_elem.get_text(strip=True) if desc_elem else title_en
                    
                    title_kr, summary_kr = get_gemini_summary(title_en, description_en)
                    
                    articles.append({
                        'title': title_kr,
                        'title_en': title_en,
                        'url': link,
                        'source': f'Nature {journal_name}',
                        'category': category,
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'summary_kr': summary_kr
                    })
                    print(f"  âœ“ {title_en[:50]}... -> {title_kr[:30]}...")
            except Exception as e:
                print(f"  âœ— í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
            time.sleep(1)
            
    except Exception as e:
        print(f"âŒ [Nature {journal_name}] í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    return articles


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ“° ì¼ì¼ ì½ì„ê±°ë¦¬ ìë™ ìˆ˜ì§‘ ë° ìš”ì•½ ì‹œì‘")
    print(f"ğŸ• ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60 + "\n")
    
    all_articles = []
    
    # ìš”ì²­í•˜ì‹  ì‚¬ì´íŠ¸ ëª©ë¡ í¬ë¡¤ë§
    all_articles.extend(scrape_nature_news())
    all_articles.extend(scrape_science_news())
    all_articles.extend(scrape_cell_news())
    all_articles.extend(scrape_thetransmitter())
    all_articles.extend(scrape_nature_journal("Neuroscience", "neuro", "Neuroscience"))
    all_articles.extend(scrape_nature_journal("Drug Discovery", "nrd", "Industry News"))
    all_articles.extend(scrape_nature_journal("Medicine", "nm", "Medical News"))
    
    # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
    seen_urls = set()
    unique_articles = []
    
    # ì´ì „ì— ë¡œë“œëœ ë°ì´í„°ë¥¼ ì½ì–´ì™€ì„œ ì¤‘ë³µ ì²´í¬ì— í™œìš© (ì„ íƒ ì‚¬í•­)
    try:
        with open('articles.json', 'r', encoding='utf-8') as f:
            old_data = json.load(f)
            for old_article in old_data.get('articles', []):
                seen_urls.add(old_article['url'])
        print(f"[i] ê¸°ì¡´ {len(seen_urls)}ê°œì˜ URLì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ê¸°ì‚¬ë§Œ ì¶”ê°€í•©ë‹ˆë‹¤.")
    except FileNotFoundError:
        print("[i] 'articles.json' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    
    
    new_article_count = 0
    for article in all_articles:
        if article['url'] not in seen_urls:
            seen_urls.add(article['url'])
            unique_articles.append(article)
            new_article_count += 1
    
    print(f"\n[i] {new_article_count}ê°œì˜ ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í•©ì¹¨ (ì„ íƒ ì‚¬í•­: ì—¬ê¸°ì„œëŠ” ìƒˆ ê¸°ì‚¬ë§Œ ì €ì¥)
    # ì—¬ê¸°ì„œëŠ” ë§¤ë²ˆ ìƒˆë¡œ ë®ì–´ì“°ëŠ” ë°©ì‹ì„ ìœ ì§€í•˜ë˜, ì¤‘ë³µ ì œê±°ëœ ì „ì²´ ëª©ë¡ì„ ì‚¬ìš©
    # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    unique_articles.sort(key=lambda x: x['date'], reverse=True)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    output = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'articles': unique_articles # ëª¨ë“  ê¸°ì‚¬ (ì¤‘ë³µ ì œê±°ë¨)
    }
    
    with open('articles.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print("\n" + "="*60)
    print(f"âœ… ì™„ë£Œ! ì´ {len(unique_articles)}ê°œ í•­ëª© ìˆ˜ì§‘ ë° ìš”ì•½")
    print(f"ğŸ“ articles.json íŒŒì¼ ì—…ë°ì´íŠ¸ë¨")
    print("="*60 + "\n")
    
    sources = {}
    for article in unique_articles:
        source = article['source']
        sources[source] = sources.get(source, 0) + 1
    
    print("ğŸ“Š ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ í˜„í™©:")
    for source, count in sources.items():
        print(f"  â€¢ {source}: {count}ê°œ")


if __name__ == '__main__':
    main()

