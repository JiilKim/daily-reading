#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¼ì¼ ê³¼í•™ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ìµœì¢… ìˆ˜ì •íŒ)
- ZoneInfoë¥¼ í†µí•œ ì •í™•í•œ Palo Alto ì‹œê°„ ì ìš©
- AI ì‘ë‹µ(List/Dict) ìœ ì—°í•œ ì²˜ë¦¬ (ì—ëŸ¬ ë°©ì§€)
- API ë° ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ì— ëŒ€í•œ ê°•í•œ ë‚´ì„± ë° ë¡œê¹…
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import feedparser
import time
import os
from google import genai
from google.genai import types
import sys
import re
# íƒ€ì„ì¡´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Python 3.9+)
try:
    from zoneinfo import ZoneInfo
except ImportError:
    # êµ¬ë²„ì „ íŒŒì´ì¬ ëŒ€ë¹„ (GitHub ActionsëŠ” ë³´í†µ ìµœì‹ ì„)
    from datetime import timezone, timedelta
    class ZoneInfo:
        def __init__(self, key): pass
        def utcoffset(self, dt): return timedelta(hours=-8)
        def tzname(self, dt): return "PST"
        def dst(self, dt): return timedelta(0)

# ============================================================================
# ì„¤ì •
# ============================================================================

MAX_NEW_ARTICLES_PER_RUN = 8000
ARCHIVE_DAYS = 99999
API_DELAY_SECONDS = 2 # API ì•ˆì •ì„±ì„ ìœ„í•´ 1ì´ˆ -> 2ì´ˆë¡œ ëŠ˜ë¦¼

# íŒ”ë¡œì•Œí†  ì‹œê°„ëŒ€ (ì¸ë¨¸íƒ€ì„ ìë™ ì ìš©)
try:
    PALO_ALTO_TZ = ZoneInfo("America/Los_Angeles")
except:
    PALO_ALTO_TZ = timezone(timedelta(hours=-8)) # fallback

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'application/xml,application/rss+xml,text/xml;q=0.9,*/*;q=0.5'
}

# ì‹¤í–‰ ë¡œê·¸ ì „ì—­ ë³€ìˆ˜
execution_logs = []

def log(message, level="INFO"):
    """ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤."""
    now = datetime.now(PALO_ALTO_TZ)
    timestamp = now.strftime('%H:%M:%S')
    
    # ì½˜ì†” ì¶œë ¥ (GitHub Actions ë¡œê·¸ìš©)
    print(f"[{timestamp}] [{level}] {message}")
    
    # íŒŒì¼ ì €ì¥ìš© ë¡œê·¸ (ì›¹ í‘œì‹œìš©)
    execution_logs.append({
        "time": timestamp,
        "level": level,
        "message": message
    })

# ============================================================================
# AI ë²ˆì—­ ë° ìš”ì•½
# ============================================================================

def clean_json_text(text):
    """JSON ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ì œ"""
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    return text.strip()

def get_gemini_summary(article_data):
    """
    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ì½˜í…ì¸ ë¥¼ ë²ˆì—­í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.
    ìœ íŠœë¸Œ ì˜ìƒì˜ ê²½ìš° URLì„ í†µí•´ ì§ì ‘ ì˜ìƒ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        article_data (dict): title_en, description_en, url, sourceë¥¼ í¬í•¨í•œ ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°
        
    Returns:
        tuple: (translated_title_kr, summary_kr)
    """
    title_en = article_data['title_en']
    description_en = article_data['description_en']
    url = article_data['url']
    source = article_data.get('source', '')

    try:
        api_key = os.environ.get('GEMINI_API_KEY')
        
        if not api_key:
            print("  [AI] âŒ GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í‚¤ ì—†ìŒ. (ì›ë³¸: {description_en[:100]}...)"

        client = genai.Client(api_key=api_key)

        # ìœ íŠœë¸Œ ì˜ìƒ: URLì„ í†µí•´ ì§ì ‘ ì˜ìƒ ì½˜í…ì¸  ë¶„ì„
        if 'YouTube' in source:
            print(f"  [AI] ğŸ¥ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ ì¤‘: '{title_en[:40]}...'")
            
            prompt = f"""
ë‹¹ì‹ ì€ ì˜ìƒ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ ìœ íŠœë¸Œ ì˜ìƒì„ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.
ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.

[ì…ë ¥]
- title_en: "{title_en}"

[JSON ì¶œë ¥ í˜•ì‹]
{{
  "title_kr": "ì—¬ê¸°ì— ì œëª©ì˜ ì „ë¬¸ì ì¸ í•œêµ­ì–´ ë²ˆì—­ì„ ì‘ì„±í•©ë‹ˆë‹¤",
  "summary_kr": "í•µì‹¬ ìš”ì ì„ ì¶”ì¶œí•˜ì—¬, ì˜ìƒ ì½˜í…ì¸ ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ìµœì†Œ 10ë¬¸ì¥ ë¶„ëŸ‰ì˜ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤"
}}

[ê·œì¹™]
1. "title_kr": "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
2. "summary_kr": ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì²´ë¡œ ìƒì„¸í•œ ìµœì†Œ 10ë¬¸ì¥ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
3. ëŒ€í™”ì²´ê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ê¸€ì“°ê¸° ë¬¸ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

            response = client.models.generate_content(
                model='gemini-2.5-flash', # ëª¨ë¸ ë²„ì „
                contents=[
                    prompt,
                    types.Part.from_uri(
                        file_uri=url,
                        mime_type="video/youtube"
                    )
                ],
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            
        # í…ìŠ¤íŠ¸ ê¸°ì‚¬: ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ë²ˆì—­ ë° ìš”ì•½
        else:
            print(f"  [AI] ğŸ“ ê¸°ì‚¬ ë²ˆì—­ ì¤‘: '{title_en[:40]}...'")
            
            prompt = f"""
ë‹¹ì‹ ì€ ê³¼í•™ì— ëŠ¥í†µí•œ ì „ë¬¸ ê¸°ì í˜¹ì€ ì»¤ë®¤ë‹ˆì¼€ì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ì˜ì–´ ê¸°ì‚¬ ì œëª©ê³¼ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ, í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
 
[ì…ë ¥]
- title_en: "{title_en}"
- description_en: "{description_en}"

[JSON ì¶œë ¥ í˜•ì‹]
{{
  "title_kr": "ì—¬ê¸°ì— í•œêµ­ì–´ ë²ˆì—­ ì œëª©ì„ ì‘ì„±",
  "summary_kr": "ì—¬ê¸°ì— ìµœì†Œ 5-6 ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ìƒì„¸í•œ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±"
}}

[ê·œì¹™]
1. "title_kr" í‚¤ì—ëŠ” "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
2. "summary_kr" í‚¤ì—ëŠ” "description_en"ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
3. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì²´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
"""
            
            response = client.models.generate_content(
                model='gemini-2.5-flash', # ëª¨ë¸ ë²„ì „
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )

        # JSON ì‘ë‹µ íŒŒì‹±
        data = json.loads(response.text)
        title_kr = data.get('title_kr', title_en)
        summary_kr = data.get('summary_kr', f"[ìš”ì•½ ì‹¤íŒ¨] API ì˜¤ë¥˜. (ì›ë³¸: {description_en[:100]}...)")

        print(f"  [AI] âœ“ ë²ˆì—­ ì™„ë£Œ: {title_kr[:40]}...")
        return title_kr, summary_kr

    except json.JSONDecodeError as e:
        print(f"  [AI] âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] ì˜ëª»ëœ API ì‘ë‹µ. (ì›ë³¸: {description_en[:100]}...)"
    
    except Exception as e:
        print(f"  [AI] âŒ API ì˜¤ë¥˜: {e}")
        return title_en, f"[ìš”ì•½ ì‹¤íŒ¨] API í˜¸ì¶œ ì‹¤íŒ¨. (ì›ë³¸: {description_en[:100]}...)"


# ============================================================================
# ìŠ¤í¬ë˜í¼
# ============================================================================

def scrape_feed(feed_url, source_name, category_name, is_youtube=False):
    articles = []
    log(f"í¬ë¡¤ë§ ì‹œì‘: {source_name}", "INFO")

    try:
        # íƒ€ì„ì•„ì›ƒ 10ì´ˆ ì„¤ì • (Wired ë“± ëŠë¦° ì‚¬ì´íŠ¸ ëŒ€ê¸° ì‹œê°„ ì œí•œ)
        response = requests.get(feed_url, headers=HEADERS, timeout=10)
        feed = feedparser.parse(response.content)
        
        palo_alto_now = datetime.now(PALO_ALTO_TZ)

        for entry in feed.entries:
            # í•„ìˆ˜ í•„ë“œ ì²´í¬
            if not entry.get('link') or not entry.get('title'):
                continue

            # ë‚ ì§œ íŒŒì‹± (Palo Alto ì‹œê°„ ê¸°ì¤€)
            published_date = palo_alto_now
            if entry.get('published_parsed'):
                try:
                    dt_utc = datetime.fromtimestamp(time.mktime(entry.published_parsed), timezone.utc)
                    published_date = dt_utc.astimezone(PALO_ALTO_TZ)
                except:
                    pass # íŒŒì‹± ì‹¤íŒ¨ì‹œ í˜„ì¬ ì‹œê°„ ìœ ì§€
            
            # 8ì¼ ì§€ë‚œ ê¸°ì‚¬ ì°¨ë‹¨ (ì¢€ë¹„ ê¸°ì‚¬ ë°©ì§€)
            days_diff = (palo_alto_now - published_date).days
            if days_diff > 8:
                continue

            date_str = published_date.strftime('%Y-%m-%d')
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            image_url = None
            if entry.get('media_thumbnail'):
                image_url = entry.media_thumbnail[0]['url']
            elif entry.get('links'):
                for link in entry.links:
                    if link.get('type', '').startswith('image/'):
                        image_url = link.get('href')
                        break
            
            # ë‚´ìš© ì¶”ì¶œ
            desc = entry.get('summary', '')
            if is_youtube:
                desc = entry.get('media_description', entry.get('summary', ''))
            
            clean_desc = BeautifulSoup(desc, 'html.parser').get_text(strip=True)

            articles.append({
                'title_en': entry.title,
                'description_en': clean_desc,
                'url': entry.link,
                'source': source_name,
                'category': category_name,
                'date': date_str,
                'image_url': image_url
            })

    except requests.exceptions.Timeout:
        log(f"{source_name}: ì—°ê²° ì‹œê°„ ì´ˆê³¼ (Timeout)", "ERROR")
    except Exception as e:
        log(f"{source_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")

    return articles

# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    start_time = datetime.now(PALO_ALTO_TZ)
    log(f"=== ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ (ë‚ ì§œ: {start_time.strftime('%Y-%m-%d')}) ===", "INFO")

    # 1. ë°ì´í„° ë¡œë“œ
    seen_urls = set()
    old_articles = []
    failed_queue = []

    try:
        with open('articles.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            # ê¸°ì¡´ ê¸°ì‚¬ ì¤‘ 7ì¼ ì´ë‚´ ê²ƒë§Œ ìœ ì§€
            for art in data.get('articles', []):
                try:
                    art_date = datetime.strptime(art.get('date', ''), '%Y-%m-%d')
                    # ë‚ ì§œ ë¹„êµ ì‹œ ì‹œê°„ëŒ€ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•´ ë‚ ì§œë§Œ ë¹„êµ
                    days_diff = (start_time.date() - art_date.date()).days
                    if days_diff <= ARCHIVE_DAYS:
                        old_articles.append(art)
                        seen_urls.add(art['url'])
                except:
                    pass # ë‚ ì§œ í˜•ì‹ ì—ëŸ¬ì‹œ ì œì™¸
            
            # ì´ì „ ì‹¤íŒ¨ ëª©ë¡ ë¡œë“œ
            failed_queue = data.get('failed_queue', [])
            
    except Exception:
        log("ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ì—†ìŒ ë˜ëŠ” ì†ìƒë¨. ìƒˆë¡œ ì‹œì‘.", "WARNING")

    # 2. ìˆ˜ì§‘ ì†ŒìŠ¤ ì •ì˜
    sources = [
        ('UCWgXoKQ4rl7SY9UHuAwxvzQ', 'B_ZCF YouTube', 'Video', True),
        ('https://www.thetransmitter.org/feed/', 'The Transmitter', 'Neuroscience', False),
        ('https://www.nature.com/nature/rss/articles?type=news', 'Nature', 'News', False),
        ('https://www.statnews.com/feed/', 'STAT News', 'News', False),
        ('https://www.the-scientist.com/atom/latest', 'The Scientist', 'News', False),
        ('https://arstechnica.com/science/feed/', 'Ars Technica', 'News', False),
        ('https://www.wired.com/feed/category/science/latest/rss', 'Wired', 'News', False),
        ('https://www.fiercebiotech.com/rss/xml', 'Fierce Biotech', 'News', False),
        ('https://endpts.com/feed/', 'Endpoints News', 'News', False),
        ('https://www.science.org/rss/news_current.xml', 'Science', 'News', False),
        ('https://www.nature.com/nature/rss/newsandcomment', 'Nature (News & Comment)', 'News', False),
        ('https://www.science.org/action/showFeed?type=etoc&feed=rss&jc=science', 'Science (Paper)', 'Paper', False),
        ('https://www.cell.com/cell/current.rss', 'Cell', 'Paper', False),
        ('https://www.nature.com/neuro/current_issue/rss', 'Nature Neuroscience', 'Paper', False),
        ('https://www.nature.com/nm/current_issue/rss', 'Nature Medicine', 'Paper', False),
        ('https://www.nature.com/nrd/current_issue/rss', 'Nature Drug Discovery', 'Paper', False),
        ('https://www.nature.com/nbt/current_issue/rss', 'Nature Biotechnology', 'Paper', False),
        ('https://www.nature.com/nature/research-articles.rss', 'Nature (Paper)', 'Paper', False),
        ('https://www.nejm.org/action/showFeed?jc=nejm&type=etoc&feed=rss', 'NEJM', 'Paper', False)
    ]

    # 3. í¬ë¡¤ë§ ë° í›„ë³´ ì„ ì •
    candidates = []
    
    # 3-1. ì‹¤íŒ¨í–ˆë˜ ê²ƒ ìš°ì„  ì¶”ê°€
    if failed_queue:
        log(f"ì§€ë‚œ ì‹¤í–‰ ì‹¤íŒ¨ í•­ëª© {len(failed_queue)}ê°œ ì¬ì‹œë„ ëŒ€ê¸°", "INFO")
        for item in failed_queue:
            if item['url'] not in seen_urls:
                candidates.append(item)

    # 3-2. ì‹ ê·œ í¬ë¡¤ë§
    for url, source, cat, is_yt in sources:
        items = scrape_feed(url, source, cat, is_yt)
        for item in items:
            if item['url'] not in seen_urls:
                candidates.append(item)

    # ì¤‘ë³µ ì œê±°
    unique_candidates = {v['url']: v for v in candidates}.values()
    log(f"ì´ ì²˜ë¦¬ ëŒ€ìƒ: {len(unique_candidates)}ê±´", "INFO")

    # 4. AI ì²˜ë¦¬
    new_articles = []
    new_failed_queue = []
    processed_cnt = 0

    for art in unique_candidates:
        if processed_cnt >= MAX_NEW_ARTICLES_PER_RUN:
            log(f"í• ë‹¹ëŸ‰({MAX_NEW_ARTICLES_PER_RUN}) ì´ˆê³¼. ë‚¨ì€ {len(unique_candidates) - processed_cnt}ê±´ì€ ë‹¤ìŒìœ¼ë¡œ ë¯¸ë£¸.", "WARNING")
            new_failed_queue.append(art)
            continue

        processed_cnt += 1
        title_kr, summary_kr = get_gemini_summary(art)

        if "[ìš”ì•½ ì‹¤íŒ¨]" in summary_kr:
            # ì‹¤íŒ¨ì‹œ íì— ì €ì¥ (ë‹¤ìŒ ì‹¤í–‰ë•Œ ìµœìš°ì„  ì²˜ë¦¬)
            new_failed_queue.append(art)
        else:
            art['title'] = title_kr
            art['summary_kr'] = summary_kr
            # ì›ë¬¸ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ (ìš©ëŸ‰ ì ˆì•½) or í•„ìš”ì‹œ art['summary_en'] = ...
            if 'description_en' in art: del art['description_en']
            new_articles.append(art)
        
        time.sleep(API_DELAY_SECONDS)

    # 5. ê²°ê³¼ ì €ì¥ (ì´ ë¶€ë¶„ì´ ê°€ì¥ ì¤‘ìš”. ì—ëŸ¬ê°€ ë‚˜ë„ ë°˜ë“œì‹œ ì €ì¥ë˜ë„ë¡ try-finallyë‚˜ ì•ˆì „ì¥ì¹˜ í•„ìš”)
    log(f"ì˜¤ëŠ˜ ì²˜ë¦¬ ê²°ê³¼: ì„±ê³µ {len(new_articles)}ê±´, ì‹¤íŒ¨/ë³´ë¥˜ {len(new_failed_queue)}ê±´", "INFO")

    final_list = old_articles + new_articles
    # ë‚ ì§œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    final_list.sort(key=lambda x: x.get('date', ''), reverse=True)

    output_data = {
        'last_updated': datetime.now(PALO_ALTO_TZ).strftime('%Y-%m-%d %H:%M:%S'),
        'logs': execution_logs, # ì—¬ê¸°ì— ì‹¤í–‰ ë¡œê·¸ í¬í•¨
        'failed_queue': new_failed_queue,
        'articles': final_list
    }

    try:
        with open('articles.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        log("ë°ì´í„° ì €ì¥ ì™„ë£Œ (articles.json)", "INFO")
    except Exception as e:
        log(f"ì¹˜ëª…ì  ì˜¤ë¥˜: íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ - {e}", "ERROR")
        # ì €ì¥ ì‹¤íŒ¨ì‹œ ë¡œê·¸ë¼ë„ ì¶œë ¥
        print(json.dumps(execution_logs, indent=2))
        sys.exit(1)

if __name__ == '__main__':
    main()
