#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¼ì¼ ê³¼í•™ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ê¸°ëŠ¥ í™•ì¥íŒ)
- ë¡œê·¸ ë‚ ì§œë³„ ë¶„ë¦¬ ì €ì¥ ë° ìƒì„¸ ë¡œê¹…
- Gemini ê¸°ë°˜ ì¼ì¼ ì¶”ì²œ ì½˜í…ì¸  ìƒì„±
- ì°¨íŠ¸ ë°ì´í„° êµ¬ì¡° ìµœì í™”
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import feedparser
import time
import os
from google import genai
from google.genai import types
import sys
import re
import traceback

# íƒ€ì„ì¡´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from zoneinfo import ZoneInfo
except ImportError:
    from datetime import timezone, timedelta
    class ZoneInfo:
        def __init__(self, key): pass
        def utcoffset(self, dt): return timedelta(hours=-8)
        def tzname(self, dt): return "PST"
        def dst(self, dt): return timedelta(0)

# ============================================================================
# ì„¤ì •
# ============================================================================

MAX_NEW_ARTICLES_PER_RUN = 8000
ARCHIVE_DAYS = 99999
API_DELAY_SECONDS = 2

try:
    PALO_ALTO_TZ = ZoneInfo("America/Los_Angeles")
except:
    PALO_ALTO_TZ = timezone(timedelta(hours=-8))

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'application/xml,application/rss+xml,text/xml;q=0.9,*/*;q=0.5'
}

# ë‚ ì§œë³„ ë¡œê·¸ ì €ì¥ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ (Dictionary)
# êµ¬ì¡°: { "2025-11-27": [ {time, level, message}, ... ] }
execution_logs_by_date = {} 
current_date_str = datetime.now(PALO_ALTO_TZ).strftime('%Y-%m-%d')

def log(message, level="INFO"):
    """ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤."""
    now = datetime.now(PALO_ALTO_TZ)
    timestamp = now.strftime('%H:%M:%S')
    
    # ì½˜ì†” ì¶œë ¥
    print(f"[{timestamp}] [{level}] {message}")
    
    # ë‚ ì§œë³„ ë¡œê·¸ ì €ì¥
    if current_date_str not in execution_logs_by_date:
        execution_logs_by_date[current_date_str] = []
        
    execution_logs_by_date[current_date_str].append({
        "time": timestamp,
        "level": level,
        "message": message
    })

# ============================================================================
# AI ê¸°ëŠ¥: ë²ˆì—­/ìš”ì•½ ë° ì¼ì¼ ì¶”ì²œ
# ============================================================================

def get_client():
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key:
        log("GEMINI_API_KEY ì—†ìŒ", "ERROR")
        return None
    return genai.Client(api_key=api_key)

def get_gemini_summary(article_data):
    """ê¸°ì‚¬ ë²ˆì—­ ë° ìš”ì•½"""
    client = get_client()
    if not client:
        return article_data['title_en'], "API í‚¤ ëˆ„ë½ìœ¼ë¡œ ìš”ì•½ ë¶ˆê°€"

    title_en = article_data['title_en']
    description_en = article_data['description_en']
    url = article_data['url']
    source = article_data.get('source', '')

    try:
        start_time = time.time()
        if 'YouTube' in source:
            print(f"  [AI] ğŸ¥ ìœ íŠœë¸Œ ì˜ìƒ ë¶„ì„ ì¤‘: '{title_en[:40]}...'")
            log(f"[AI] ì˜ìƒ ë¶„ì„ ìš”ì²­: {title_en[:30]}...", "DETAIL")
            prompt = f"""
            ë‹¹ì‹ ì€ ì˜ìƒ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ ìœ íŠœë¸Œ ì˜ìƒì„ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.
            ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.
            
            [ì…ë ¥]
            - title_en: "{title_en}"
            
            [JSON ì¶œë ¥ í˜•ì‹]
            {{
              "title_kr": "ì—¬ê¸°ì— ì œëª©ì˜ ì „ë¬¸ì ì¸ í•œêµ­ì–´ ë²ˆì—­ì„ ì‘ì„±í•©ë‹ˆë‹¤",
              "summary_kr": "í•µì‹¬ ìš”ì ì„ ì¶”ì¶œí•˜ì—¬, ì˜ìƒ ì½˜í…ì¸ ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ìµœì†Œ 10ë¬¸ì¥ ë¶„ëŸ‰ì˜ í•œêµ­ì–´ ìš”ì•½ë¬¸ì„ ì‘ì„±í•©ë‹ˆë‹¤"
            }}
            
            [ê·œì¹™]
            1. "title_kr": "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
            2. "summary_kr": ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì²´ë¡œ ìƒì„¸í•œ ìµœì†Œ 10ë¬¸ì¥ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
            3. ëŒ€í™”ì²´ê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ê¸€ì“°ê¸° ë¬¸ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            """
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=[prompt, types.Part.from_uri(file_uri=url, mime_type="video/youtube")],
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
        else:
            log(f"[AI] í…ìŠ¤íŠ¸ ìš”ì•½ ìš”ì²­: {title_en[:30]}...", "DETAIL")
            prompt = f"""
            ë‹¹ì‹ ì€ ê³¼í•™ì— ëŠ¥í†µí•œ ì „ë¬¸ ê¸°ì í˜¹ì€ ì»¤ë®¤ë‹ˆì¼€ì´í„°ì…ë‹ˆë‹¤.
            ì•„ë˜ì˜ ì˜ì–´ ê¸°ì‚¬ ì œëª©ê³¼ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ, í•œêµ­ì–´ ì œëª©ê³¼ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
             
            [ì…ë ¥]
            - title_en: "{title_en}"
            - description_en: "{description_en}"
            
            [JSON ì¶œë ¥ í˜•ì‹]
            {{
              "title_kr": "ì—¬ê¸°ì— í•œêµ­ì–´ ë²ˆì—­ ì œëª©ì„ ì‘ì„±",
              "summary_kr": "ì—¬ê¸°ì— ìµœì†Œ 5-6 ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ìƒì„¸í•œ í•œêµ­ì–´ ìš”ì•½ë³¸ì„ ì‘ì„±"
            }}
            
            [ê·œì¹™]
            1. "title_kr" í‚¤ì—ëŠ” "title_en"ì„ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ì œëª©ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
            2. "summary_kr" í‚¤ì—ëŠ” "description_en"ì˜ í•µì‹¬ ë‚´ìš©ì„ ìƒì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
            3. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì²´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
            """
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
        
        elapsed = time.time() - start_time
        data = json.loads(response.text)
        log(f"[AI] ì²˜ë¦¬ ì™„ë£Œ ({elapsed:.2f}s): {data.get('title_kr', '')[:20]}...", "INFO")
        
        return data.get('title_kr', title_en), data.get('summary_kr', "ìš”ì•½ ìƒì„± ì‹¤íŒ¨")

    except Exception as e:
        log(f"[AI] ì—ëŸ¬ ë°œìƒ: {str(e)}", "ERROR")
        return title_en, f"[ì‹œìŠ¤í…œ ì—ëŸ¬] ìš”ì•½ ì‹¤íŒ¨: {str(e)}"

def generate_daily_recommendations(articles_today):
    """ì˜¤ëŠ˜ì˜ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¬í™”/ê´€ë ¨ ì¶”ì²œ ì½˜í…ì¸  ìƒì„± (ì•½ 100ê°œ ëª©í‘œ)"""
    client = get_client()
    if not client or not articles_today:
        return []

    titles = [a['title'] for a in articles_today[:50]] # í† í° ì œí•œ ê³ ë ¤í•˜ì—¬ ìƒìœ„ 50ê°œë§Œ ì°¸ì¡°
    titles_text = "\n".join(titles)

    log(f"[AI] ì˜¤ëŠ˜ì˜ ì¶”ì²œ ì½˜í…ì¸  ìƒì„± ì‹œì‘ (ì°¸ì¡° ê¸°ì‚¬ {len(titles)}ê±´)", "INFO")

    prompt = f"""
    ë‹¹ì‹ ì€ ê³¼í•™ ì „ë¬¸ íë ˆì´í„°ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ì£¼ìš” ê³¼í•™ ê¸°ì‚¬ì˜ ì œëª©ë“¤ì…ë‹ˆë‹¤.
    ì´ ì£¼ì œë“¤ê³¼ ê´€ë ¨í•˜ì—¬, ë…ìë“¤ì´ ë” ì½ì–´ë³¼ ë§Œí•œ 'ê´€ë ¨ì„± ë†’ê³  ì¸ê¸° ìˆëŠ” ì›¹ì‚¬ì´íŠ¸ í˜¹ì€ ê¸°ì‚¬'ë¥¼ **ìµœëŒ€í•œ ë§ì´(ëª©í‘œ: 50~100ê°œ)** ì¶”ì²œí•´ ì£¼ì„¸ìš”.
    
    [ì˜¤ëŠ˜ì˜ ê¸°ì‚¬ ì£¼ì œ]
    {titles_text}

    [ìš”êµ¬ì‚¬í•­]
    1. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    2. ê° ì¶”ì²œ í•­ëª©ì€ {{ "title": "ì œëª©", "url": "URL(ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë§í¬ì—¬ì•¼ í•¨, ëª¨ë¥´ë©´ ê²€ìƒ‰ í‚¤ì›Œë“œ ê¸°ë°˜ì˜ êµ¬ê¸€ ê²€ìƒ‰ ë§í¬ ìƒì„±)", "description": "í•œê¸€ ì„¤ëª…(1-2ë¬¸ì¥)", "category": "ë¶„ì•¼" }} í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
    3. URLì€ í• ë£¨ì‹œë„¤ì´ì…˜ì„ í”¼í•˜ê¸° ìœ„í•´, í™•ì‹¤í•˜ì§€ ì•Šë‹¤ë©´ `https://www.google.com/search?q=í‚¤ì›Œë“œ` í˜•íƒœë¡œ ì‘ì„±í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.
    4. í•œêµ­ì–´ ì„¤ëª…ì€ ì¹œê·¼í•˜ê³  ìœ ìµí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    """

    try:
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=types.GenerateContentConfig(response_mime_type="application/json")
        )
        recommendations = json.loads(response.text)
        if isinstance(recommendations, list):
            log(f"[AI] ì¶”ì²œ ì½˜í…ì¸  {len(recommendations)}ê°œ ìƒì„± ì™„ë£Œ", "INFO")
            return recommendations
        elif isinstance(recommendations, dict) and 'recommendations' in recommendations:
             log(f"[AI] ì¶”ì²œ ì½˜í…ì¸  {len(recommendations['recommendations'])}ê°œ ìƒì„± ì™„ë£Œ", "INFO")
             return recommendations['recommendations']
        else:
            log("[AI] ì¶”ì²œ ì½˜í…ì¸  í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ", "WARNING")
            return []
    except Exception as e:
        log(f"[AI] ì¶”ì²œ ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")
        return []

# ============================================================================
# ìŠ¤í¬ë˜í¼ ë¡œì§
# ============================================================================

def scrape_feed(feed_url, source_name, category_name, is_youtube):
    articles = []
    log(f"[í¬ë¡¤ë§] ì†ŒìŠ¤ ì ‘ê·¼: {source_name}", "INFO")

    try:
        response = requests.get(feed_url, headers=HEADERS, timeout=15)
        feed = feedparser.parse(response.content)
        
        palo_alto_now = datetime.now(PALO_ALTO_TZ)
        count = 0

        for entry in feed.entries:
            if not entry.get('link') or not entry.get('title'): continue

            published_date = palo_alto_now
            if entry.get('published_parsed'):
                try:
                    dt_utc = datetime.fromtimestamp(time.mktime(entry.published_parsed), timezone.utc)
                    published_date = dt_utc.astimezone(PALO_ALTO_TZ)
                except: pass
            
            days_diff = (palo_alto_now - published_date).days
            if days_diff > 8: continue

            date_str = published_date.strftime('%Y-%m-%d')
            
            image_url = None
            if entry.get('media_thumbnail'):
                image_url = entry.media_thumbnail[0]['url']
            elif entry.get('links'):
                for link in entry.links:
                    if link.get('type', '').startswith('image/'):
                        image_url = link.get('href'); break
            
            desc = entry.get('summary', '')
            if is_youtube: desc = entry.get('media_description', entry.get('summary', ''))
            clean_desc = BeautifulSoup(desc, 'html.parser').get_text(strip=True)

            articles.append({
                'title_en': entry.title,
                'description_en': clean_desc,
                'url': entry.link,
                'source': source_name,
                'category': category_name,
                'date': date_str,
                'image_url': image_url
            })
            count += 1
        
        log(f"[í¬ë¡¤ë§] {source_name}: {count}ê°œ í•­ëª© ìˆ˜ì§‘", "INFO")

    except Exception as e:
        log(f"[í¬ë¡¤ë§] {source_name} ì‹¤íŒ¨: {str(e)}", "ERROR")

    return articles

def scrape_youtube_videos(channel_id, source_name, category_name):
    articles = []
    log(f"[í¬ë¡¤ë§] ìœ íŠœë¸Œ ì±„ë„ íƒìƒ‰: {source_name} ({channel_id})", "INFO")
    feed_url = f'https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}'

    try:
        response = requests.get(feed_url, headers=HEADERS, timeout=20)
        feed = feedparser.parse(response.content)
        
        count = 0
        for entry in feed.entries:
            if not entry.get('title') or not entry.get('link'): continue
            
            link = entry.link
            date_str = datetime.now(PALO_ALTO_TZ).strftime('%Y-%m-%d') # RSSì—ëŠ” ì •í™•í•œ ì‹œê°„ ì—†ì„ ìˆ˜ ìˆìŒ
            if entry.get('published_parsed'):
                dt_obj = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                date_str = dt_obj.strftime('%Y-%m-%d')

            image_url = None
            if entry.get('media_thumbnail'):
                image_url = entry.media_thumbnail[0]['url'].replace('default.jpg', 'hqdefault.jpg')

            description_text = BeautifulSoup(entry.get('media_description', ''), 'html.parser').get_text(strip=True)

            articles.append({
                'title_en': entry.title,
                'description_en': description_text,
                'url': link,
                'source': source_name,
                'category': category_name,
                'date': date_str,
                'image_url': image_url
            })
            count += 1
        
        log(f"[í¬ë¡¤ë§] {source_name}: {count}ê°œ ì˜ìƒ ë°œê²¬", "INFO")

    except Exception as e:
        log(f"[í¬ë¡¤ë§] ìœ íŠœë¸Œ ì˜¤ë¥˜: {e}", "ERROR")

    return articles

# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    global execution_logs_by_date
    start_time = datetime.now(PALO_ALTO_TZ)
    today_str = start_time.strftime('%Y-%m-%d')
    
    log(f"=== ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œì‘ ===", "INFO")
    log(f"ì‹¤í–‰ í™˜ê²½: Palo Alto Time {start_time}", "DETAIL")

    # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    seen_urls = set()
    old_articles = []
    failed_queue = []
    daily_recommendations = {} # ë‚ ì§œë³„ ì¶”ì²œ ì €ì¥ì†Œ { "2025-11-27": [...] }

    if os.path.exists('articles.json'):
        try:
            with open('articles.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # ê¸°ì‚¬ ë¡œë“œ
                for art in data.get('articles', []):
                    # ë‚ ì§œ ë¹„êµ ë¡œì§ (ë‹¨ìˆœí™”)
                    if art.get('date'):
                        old_articles.append(art)
                        seen_urls.add(art['url'])
                
                # ì‹¤íŒ¨ í ë¡œë“œ
                failed_queue = data.get('failed_queue', [])
                
                # ê¸°ì¡´ ë¡œê·¸ ë¡œë“œ (ë‚ ì§œë³„ êµ¬ì¡°ë¡œ ë˜ì–´ìˆë‹¤ê³  ê°€ì •, ì•„ë‹ˆë©´ ë§ˆì´ê·¸ë ˆì´ì…˜)
                loaded_logs = data.get('logs', {})
                if isinstance(loaded_logs, list): # êµ¬ë²„ì „(ë¦¬ìŠ¤íŠ¸) í˜¸í™˜
                    log("êµ¬ë²„ì „ ë¡œê·¸ í˜•ì‹ì„ ê°ì§€í•˜ì—¬ ë‚ ì§œë³„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.", "WARNING")
                    # êµ¬ë²„ì „ ë¡œê·¸ëŠ” ë³´ì¡´í•˜ì§€ ì•Šê±°ë‚˜ ì˜¤ëŠ˜ ë‚ ì§œë¡œ í¸ì… (ì—¬ê¸°ì„  ë‹¨ìˆœí™” ìœ„í•´ ìƒëµí•˜ê±°ë‚˜ ë³„ë„ ì²˜ë¦¬ ê°€ëŠ¥)
                else:
                    execution_logs_by_date.update(loaded_logs)
                
                # ê¸°ì¡´ ì¶”ì²œ ëª©ë¡ ë¡œë“œ
                daily_recommendations = data.get('recommendations', {})

        except Exception as e:
            log(f"ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")

    # 2. ì†ŒìŠ¤ ì •ì˜
    sources = [
        ('https://www.thetransmitter.org/feed/', 'The Transmitter', 'Neuroscience', False),
        ('https://www.nature.com/nature/rss/articles?type=news', 'Nature', 'News', False),
        ('https://www.statnews.com/feed/', 'STAT News', 'News', False),
        ('https://www.the-scientist.com/atom/latest', 'The Scientist', 'News', False),
        ('https://arstechnica.com/science/feed/', 'Ars Technica', 'News', False),
        ('https://www.wired.com/feed/category/science/latest/rss', 'Wired', 'News', False),
        ('https://www.fiercebiotech.com/rss/xml', 'Fierce Biotech', 'News', False),
        ('https://endpts.com/feed/', 'Endpoints News', 'News', False),
        ('https://www.science.org/rss/news_current.xml', 'Science', 'News', False),
        ('https://www.nature.com/nature/rss/newsandcomment', 'Nature (News & Comment)', 'News', False),
        ('https://www.science.org/action/showFeed?type=etoc&feed=rss&jc=science', 'Science (Paper)', 'Paper', False),
        ('https://www.cell.com/cell/current.rss', 'Cell', 'Paper', False),
        ('https://www.nature.com/neuro/current_issue/rss', 'Nature Neuroscience', 'Paper', False),
        ('https://www.nature.com/nm/current_issue/rss', 'Nature Medicine', 'Paper', False),
        ('https://www.nature.com/nrd/current_issue/rss', 'Nature Drug Discovery', 'Paper', False),
        ('https://www.nature.com/nbt/current_issue/rss', 'Nature Biotechnology', 'Paper', False),
        ('https://www.nature.com/nature/research-articles.rss', 'Nature (Paper)', 'Paper', False),
        ('https://www.nejm.org/action/showFeed?jc=nejm&type=etoc&feed=rss', 'NEJM', 'Paper', False)
    ]

    # 3. í¬ë¡¤ë§
    candidates = []
    
    # ì¬ì‹œë„ í
    if failed_queue:
        log(f"ì¬ì‹œë„ íì—ì„œ {len(failed_queue)}ê°œ í•­ëª© ë¡œë“œ", "INFO")
        for item in failed_queue:
            if item['url'] not in seen_urls: candidates.append(item)

    # ìœ íŠœë¸Œ
    candidates.extend(scrape_youtube_videos('UCWgXoKQ4rl7SY9UHuAwxvzQ', 'B_ZCF YouTube', 'Video'))

    # RSS ì†ŒìŠ¤
    for url, source, cat, is_yt in sources:
        items = scrape_feed(url, source, cat, is_yt)
        for item in items:
            if item['url'] not in seen_urls: candidates.append(item)

    # ì¤‘ë³µ ì œê±°
    unique_candidates = {v['url']: v for v in candidates}.values()
    log(f"ì²˜ë¦¬ ëŒ€ìƒ: ì´ {len(unique_candidates)}ê±´", "INFO")

    # 4. AI ì²˜ë¦¬ (ë²ˆì—­/ìš”ì•½)
    new_articles = []
    new_failed_queue = []
    processed_cnt = 0

    for art in unique_candidates:
        if processed_cnt >= MAX_NEW_ARTICLES_PER_RUN:
            new_failed_queue.append(art); continue

        processed_cnt += 1
        title_kr, summary_kr = get_gemini_summary(art)

        if "[ìš”ì•½ ì‹¤íŒ¨]" in summary_kr or "[ì‹œìŠ¤í…œ ì—ëŸ¬]" in summary_kr:
            new_failed_queue.append(art)
            log(f"ì²˜ë¦¬ ì‹¤íŒ¨: {art['title_en'][:20]}...", "WARNING")
        else:
            art['title'] = title_kr
            art['summary_kr'] = summary_kr
            if 'description_en' in art: del art['description_en']
            new_articles.append(art)
        
        time.sleep(API_DELAY_SECONDS)

    # 5. ì¼ì¼ ì¶”ì²œ ì½˜í…ì¸  ìƒì„± (ì˜¤ëŠ˜ ìƒˆë¡œ ì¶”ê°€ëœ ê¸°ì‚¬ê°€ ìˆì„ ê²½ìš°)
    if new_articles:
        log(f"ì˜¤ëŠ˜ì˜ ì‹ ê·œ ê¸°ì‚¬ {len(new_articles)}ê±´ì— ëŒ€í•œ ì¶”ì²œ ì½˜í…ì¸  ìƒì„± ì¤‘...", "INFO")
        todays_recs = generate_daily_recommendations(new_articles)
        if todays_recs:
            daily_recommendations[today_str] = todays_recs
    else:
        log("ì‹ ê·œ ê¸°ì‚¬ê°€ ì—†ì–´ ì¶”ì²œ ì½˜í…ì¸  ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.", "INFO")

    # 6. ì €ì¥
    log(f"ì‘ì—… ì™„ë£Œ: ì„±ê³µ {len(new_articles)}ê±´, ë³´ë¥˜ {len(new_failed_queue)}ê±´", "INFO")

    final_list = old_articles + new_articles
    final_list.sort(key=lambda x: x.get('date', ''), reverse=True)

    # í˜„ì¬ ë©”ëª¨ë¦¬ì— ìˆëŠ” ë¡œê·¸ë¥¼ ì €ì¥ êµ¬ì¡°ì— ë°˜ì˜
    # execution_logs_by_dateëŠ” ì´ë¯¸ ì „ì—­ë³€ìˆ˜ë¡œì„œ log() í•¨ìˆ˜ì— ì˜í•´ ì—…ë°ì´íŠ¸ë¨

    output_data = {
        'last_updated': datetime.now(PALO_ALTO_TZ).strftime('%Y-%m-%d %H:%M:%S'),
        'logs': execution_logs_by_date, # ë‚ ì§œë³„ ë¡œê·¸ ê°ì²´
        'failed_queue': new_failed_queue,
        'articles': final_list,
        'recommendations': daily_recommendations # ì¶”ì²œ ë°ì´í„° ì¶”ê°€
    }

    try:
        with open('articles.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        log("articles.json ì €ì¥ ì™„ë£Œ", "INFO")
    except Exception as e:
        log(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}", "ERROR")
        sys.exit(1)

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        log(f"ì¹˜ëª…ì  ìŠ¤í¬ë¦½íŠ¸ ì˜¤ë¥˜: {traceback.format_exc()}", "ERROR")
        sys.exit(1)
